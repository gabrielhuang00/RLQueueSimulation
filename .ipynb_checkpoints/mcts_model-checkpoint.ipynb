{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf76f890-ae8c-44bf-87aa-a4a5cc358025",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import heapq\n",
    "import torch.multiprocessing as mp\n",
    "from collections import deque, OrderedDict\n",
    "from typing import List, Dict, Tuple, Any, Optional\n",
    "from mcts_NetworkClasses import (\n",
    "    Network, \n",
    "    ExtendedSixClassNetwork,\n",
    "    SchedulingPolicy,\n",
    "    Server,\n",
    "    Queue,\n",
    "    EventType,\n",
    "    LBFSPolicy,\n",
    "    FIFONetPolicy\n",
    ")\n",
    "\n",
    "# --- Hyperparameters / Config ---\n",
    "CONFIG = {\n",
    "    \"L\": 2,                       \n",
    "    \"MAX_QUEUES_STATE\": 6,       \n",
    "    \"MAX_STATIONS\": 2,            \n",
    "    \"MAX_QUEUES_PER_STATION\": 3,  \n",
    "    \n",
    "    \"learning_rate\": 0.001,\n",
    "    \"buffer_size\": 100000,\n",
    "    \"batch_size\": 2048, #2048 \n",
    "    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "\n",
    "    \"mcts_simulations\": 350, \n",
    "    \"c_puct\": 1.5,\n",
    "    \"mcts_sim_horizon_s\": 100.0,\n",
    "    \"temperature\": 0.3,\n",
    "    \"dirichlet_alpha\": 0.3,   \n",
    "    \"dirichlet_epsilon\": 0.1,\n",
    "    \"discount_factor\": 0.97,  # Experiment.\n",
    "    \n",
    "    \"num_train_loops\": 100,\n",
    "    \"episodes_per_loop\": 35,     #35 \n",
    "    \"train_steps_per_loop\": 100, #120\n",
    "    \"sim_run_duration\": 4000.0, #4000 \n",
    "    \"CATASTROPHE_SOJOURN_TIME\": 50.0, \n",
    "    \"seed\": 1\n",
    "}\n",
    "\n",
    "# --- 1. The Neural Network ---\n",
    "class AlphaZeroNN(nn.Module):\n",
    "    def __init__(self, state_size, action_space_size):\n",
    "        super(AlphaZeroNN, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_space_size = action_space_size\n",
    "        \n",
    "        self.body = nn.Sequential(\n",
    "            nn.Linear(self.state_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.policy_head = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, self.action_space_size)\n",
    "        )\n",
    "        \n",
    "        self.value_head = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1), \n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        body_out = self.body(state)\n",
    "        policy_logits = self.policy_head(body_out)\n",
    "        value = self.value_head(body_out)\n",
    "        return policy_logits, value\n",
    "\n",
    "# --- 2. The Replay Buffer ---\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size):\n",
    "        self.buffer = deque(maxlen=buffer_size)\n",
    "\n",
    "    def push(self, state, policy_target, value_target, action_mask):\n",
    "        self.buffer.append((state, policy_target, value_target, action_mask))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, policy_targets, value_targets, action_mask = zip(*batch)\n",
    "        return (np.array(states), \n",
    "                np.array(policy_targets), \n",
    "                np.array(value_targets).reshape(-1, 1),\n",
    "               np.array(action_mask))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# --- 3. The MCTS Node ---\n",
    "class MCTSNode:\n",
    "    def __init__(self, parent: Optional[MCTSNode] = None, prior_p: float = 0.0):\n",
    "        self.parent = parent\n",
    "        self.children: Dict[int, MCTSNode] = {} \n",
    "        \n",
    "        self.N = 0      \n",
    "        self.W = 0.0    \n",
    "        self.Q = 0.0    \n",
    "        self.P = prior_p  \n",
    "        \n",
    "        self._state_vector: Optional[np.ndarray] = None\n",
    "        self._is_terminal: bool = False\n",
    "\n",
    "    def detach(self):\n",
    "        \"\"\"\n",
    "        Manually breaks the cycle between parent and children\n",
    "        to allow instant garbage collection.\n",
    "        \"\"\"\n",
    "        for child in self.children.values():\n",
    "            child.detach()\n",
    "        self.children.clear()\n",
    "        self.parent = None\n",
    "        self._state_vector = None\n",
    "\n",
    "\n",
    "    def get_state_vector(self, sim_net: Network, max_queues: int, context_server_idx: int = -1, L: int = 2) -> np.ndarray:\n",
    "        if self._state_vector is None:\n",
    "            # 1. Get Queue Lengths (as a standard list)\n",
    "            q_lengths_dict = sim_net.total_queue_lengths()\n",
    "            ordered_q_lengths = OrderedDict(sorted(q_lengths_dict.items()))\n",
    "            state = [length for length in ordered_q_lengths.values()]\n",
    "            \n",
    "            # 2. Pad the List (List + List is safe!)\n",
    "            # We pad with 0s here. Since log1p(0) = 0, this works perfectly.\n",
    "            padding = [0] * (max_queues - len(state))\n",
    "            padded_state = state + padding\n",
    "            \n",
    "            # 3. Log Normalize the Padded List\n",
    "            # This creates [log(q1), log(q2)..., 0.0, 0.0]\n",
    "            log_queues = np.log1p(np.array(padded_state, dtype=np.float32))\n",
    "\n",
    "            # 4. Create Station Context (One-Hot)\n",
    "            server_one_hot = np.zeros(L, dtype=np.float32)\n",
    "            if context_server_idx >= 0 and context_server_idx < L:\n",
    "                server_one_hot[context_server_idx] = 1.0\n",
    "            # 5. Concatenate: [Queues..., Padding..., Servers...]\n",
    "            self._state_vector = np.concatenate([log_queues, server_one_hot])\n",
    "            \n",
    "        return self._state_vector\n",
    "\n",
    "    def expand(self, policy_probs: np.ndarray):\n",
    "        for action_idx, prob in enumerate(policy_probs):\n",
    "            if prob > 0: \n",
    "                if action_idx not in self.children:\n",
    "                    self.children[action_idx] = MCTSNode(parent=self, prior_p=prob)\n",
    "\n",
    "    #Problem here was that the exploitation term completely destroys NN's suggestion. Fix implemented below.\n",
    "    def select_child_puct(self, c_puct: float) -> Tuple[int, MCTSNode]:\n",
    "        best_score = -np.inf\n",
    "        best_action_idx = -1\n",
    "        best_child = None\n",
    "        sqrt_self_N = math.sqrt(self.N)\n",
    "        \n",
    "        for action_idx, child in self.children.items():         \n",
    "            score = child.Q + c_puct * child.P * (sqrt_self_N / (1 + child.N))\n",
    "            \n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_action_idx = action_idx\n",
    "                best_child = child\n",
    "                \n",
    "        return best_action_idx, best_child\n",
    "\n",
    "    def update_stats(self, value: float):\n",
    "        self.N += 1\n",
    "        self.W += value\n",
    "        self.Q = self.W / self.N\n",
    "\n",
    "# --- 4. The MCTS Policy ---\n",
    "class MCTS_Policy(SchedulingPolicy):\n",
    "    def __init__(self, model: AlphaZeroNN, config: Dict[str, Any]):\n",
    "        self.model = model\n",
    "        self.config = config\n",
    "        self.device = config[\"device\"]\n",
    "        self.master_action_list = self._build_master_action_list()\n",
    "        self.action_to_idx = {action: i for i, action in enumerate(self.master_action_list)}\n",
    "        self.idx_to_action = {i: action for i, action in enumerate(self.master_action_list)}\n",
    "        self.NULL_ACTION_IDX = self.action_to_idx[(\"NULL\", \"NULL\")]\n",
    "\n",
    "    def _build_master_action_list(self) -> List[Tuple[str, str]]:\n",
    "        actions = []\n",
    "        for i in range(1, self.config[\"L\"] + 1):\n",
    "            station_id = f\"S{i}\"\n",
    "            server_id = f\"{station_id}-s0\"\n",
    "            for k in range(1, 4):\n",
    "                class_id = 3 * (i - 1) + k\n",
    "                queue_id = f\"Q{class_id}\"\n",
    "                actions.append((server_id, queue_id))\n",
    "        actions.append((\"NULL\", \"NULL\"))\n",
    "        return actions\n",
    "\n",
    "    def _get_server_context_idx(self, station_id: str) -> int:\n",
    "        # Assumes station_id is \"S1\", \"S2\", etc.\n",
    "        # Returns 0 for S1, 1 for S2...\n",
    "        return int(station_id.replace(\"S\", \"\")) - 1\n",
    "\n",
    "    \n",
    "    def _get_action_mask(self, sim_net: Network, free_servers: Dict[str, List[Server]]) -> np.ndarray:\n",
    "        mask = np.zeros(len(self.master_action_list), dtype=int)\n",
    "        \n",
    "        # 1. Identify which queues have jobs\n",
    "        non_empty_queues = {\n",
    "            (sid, qid) for sid, st in sim_net.stations.items()\n",
    "            for qid, q in st.queues.items() if len(q) > 0\n",
    "        }\n",
    "        \n",
    "        found_legal_action = False\n",
    "        \n",
    "        # 2. Find all valid server-to-queue assignments\n",
    "        for station_id, srvs in free_servers.items():\n",
    "            for srv in srvs:\n",
    "                for action_idx, (srv_id, qid) in enumerate(self.master_action_list):\n",
    "                    # STRICT MATCH: Ensure Action Server ID == Real Server ID\n",
    "                    if srv.server_id == srv_id:\n",
    "                        if (station_id, qid) in non_empty_queues:\n",
    "                            mask[action_idx] = 1\n",
    "                            found_legal_action = True\n",
    "                            \n",
    "        # 3. ONLY allow NULL if we found NO other legal actions\n",
    "        # This forces the policy to be \"Work Conserving\"\n",
    "        if not found_legal_action:\n",
    "            mask[self.NULL_ACTION_IDX] = 1\n",
    "            \n",
    "        return mask\n",
    "    \n",
    "    def _compute_score(self, mean_sys_size: float) -> float:\n",
    "        cat_val = self.config[\"CATASTROPHE_SOJOURN_TIME\"]\n",
    "        log_val = np.log1p(mean_sys_size)\n",
    "        ref_log = np.log1p(cat_val)\n",
    "        final_score = 1.0 - (2.0 * (log_val / ref_log))\n",
    "        return final_score\n",
    "\n",
    "    def decide(self, net: Network, t: float, free_servers: Dict[str, List[Server]]) -> Tuple[Dict[Server, Queue], MCTSNode, np.ndarray, float]:\n",
    "        #Find server to assign.\n",
    "        server_to_assign = None\n",
    "        station_id_for_server = None \n",
    "        action_maskx = self._get_action_mask(net, free_servers)\n",
    "        for st_id, srvs in free_servers.items():\n",
    "            if srvs:\n",
    "                server_to_assign = srvs[0]\n",
    "                station_id_for_server = st_id     \n",
    "                break\n",
    "        \n",
    "        #Check if there are any jobs.\n",
    "        target_station = net.stations[station_id_for_server]\n",
    "        target_station_id = station_id_for_server\n",
    "        jobs_at_station = sum(len(q) for q in target_station.queues.values())  \n",
    "        root_srv_idx = self._get_server_context_idx(target_station_id)\n",
    "\n",
    "        if jobs_at_station == 0 or server_to_assign is None:\n",
    "            root = MCTSNode()\n",
    "            # Get the correct context index for the neural net input (e.g., S1 -> 0, S2 -> 1)\n",
    "            state_vec = root.get_state_vector(net, self.config[\"MAX_QUEUES_STATE\"], root_srv_idx)\n",
    "            return {}, root, state_vec, 0.0, action_maskx\n",
    "        \n",
    "        #If only there's one free queue, take it.\n",
    "        occupied = 0\n",
    "        o_queues = []\n",
    "        for q in target_station.queues.values():\n",
    "            if len(q) >= 1: \n",
    "                occupied += 1\n",
    "                o_queues.append(q)\n",
    "        if occupied == 1:\n",
    "            root = MCTSNode()\n",
    "            state_vec = root.get_state_vector(net, self.config[\"MAX_QUEUES_STATE\"], root_srv_idx)\n",
    "            return {server_to_assign: o_queues[0]}, root, state_vec, 0.0, action_maskx\n",
    "            \n",
    "        #Make sure the network is cloneable. \n",
    "        try:\n",
    "            real_snapshot = net.clone()\n",
    "        except:\n",
    "            root = MCTSNode()\n",
    "            state_vec = root.get_state_vector(net, self.config[\"MAX_QUEUES_STATE\"], root_srv_idx)\n",
    "            return {}, root, state_vec, 0.0, action_maskx\n",
    "\n",
    "        # ---- BEGIN ACTUAL MCTS -----\n",
    "        #Get one-hot encoding for free station.\n",
    "        root_srv_idx = -1\n",
    "        if station_id_for_server:\n",
    "            root_srv_idx = self._get_server_context_idx(station_id_for_server)\n",
    "\n",
    "        root = MCTSNode()\n",
    "        state_vec = root.get_state_vector(net, self.config[\"MAX_QUEUES_STATE\"], root_srv_idx)\n",
    "        search_free_servers = {station_id_for_server: [server_to_assign]}\n",
    "        sim_count = 0\n",
    "        for _ in range(self.config[\"mcts_simulations\"]):\n",
    "\n",
    "            #--- DEBUG PRINT --- Root Children Stats\n",
    "            # vc = []\n",
    "            # for child in root.children.items():\n",
    "            #     vc.append(child[1].N)\n",
    "            # print(vc)\n",
    "            \n",
    "            node = root\n",
    "            sim_net = real_snapshot.clone()\n",
    "            sim_t = t\n",
    "            sim_free_servers = search_free_servers\n",
    "            \n",
    "            path = [node]\n",
    "            step_rewards = []\n",
    "            actions = []\n",
    "            while node.children: \n",
    "                \n",
    "                action_idx, node = node.select_child_puct(self.config[\"c_puct\"])\n",
    "                path.append(node)\n",
    "                actions.append(self.idx_to_action[action_idx])\n",
    "                #print(\"1. idx_to_action\",self.idx_to_action[action_idx], action_idx)\n",
    "                (sim_net, sim_t, sim_free_servers, is_terminal, step_reward) = self._run_sim_step(sim_net, self.idx_to_action[action_idx])\n",
    "                step_rewards.append(step_reward)\n",
    "                if is_terminal:\n",
    "                    node._is_terminal = True\n",
    "                    break\n",
    "            value = 0.0 \n",
    "\n",
    "            if not node._is_terminal:\n",
    "                \n",
    "                leaf_srv_idx = -1\n",
    "                if sim_free_servers:\n",
    "                    # Pick the first available station/server at the leaf state\n",
    "                    #print(\"THERE ARE FREE SERVERS\", sim_free_servers)\n",
    "                    first_st_id = next(iter(sim_free_servers))\n",
    "                    leaf_srv_idx = self._get_server_context_idx(first_st_id)\n",
    "                    \n",
    "                leaf_state_vec = node.get_state_vector(sim_net, self.config[\"MAX_QUEUES_STATE\"], leaf_srv_idx)\n",
    "                state_tensor = torch.tensor(leaf_state_vec, dtype=torch.float32).unsqueeze(0).to(self.device)\n",
    "                with torch.no_grad():\n",
    "                    policy_logits, value_tensor = self.model(state_tensor)\n",
    "                policy_logits = policy_logits.squeeze(0)\n",
    "                value = value_tensor.item()\n",
    "                \n",
    "                action_mask = self._get_action_mask(sim_net, sim_free_servers)\n",
    "                policy_logits[action_mask == 0] = -torch.inf\n",
    "                policy_probs = F.softmax(policy_logits, dim=0).cpu().numpy()\n",
    "                #print(\"policy_probs\", policy_probs)\n",
    "                \n",
    "                if node is root:\n",
    "                    num_legal = np.count_nonzero(action_mask)\n",
    "                    if num_legal > 0:\n",
    "                        noise = np.random.dirichlet([self.config[\"dirichlet_alpha\"]] * num_legal)\n",
    "                        eps = self.config[\"dirichlet_epsilon\"]\n",
    "                        idx = 0\n",
    "                        for i in range(len(policy_probs)):\n",
    "                            if action_mask[i]:\n",
    "                                policy_probs[i] = (1 - eps) * policy_probs[i] + eps * noise[idx]\n",
    "                                idx += 1\n",
    "                                \n",
    "                node.expand(policy_probs)\n",
    "                \n",
    "            # # Backprop with Standard Accumulation\n",
    "            gamma = self.config[\"discount_factor\"]\n",
    "            curr_val = value \n",
    "            path[-1].update_stats(value)\n",
    "            for i in range(len(path) - 2, -1, -1):\n",
    "                parent = path[i]\n",
    "                curr_val = step_rewards[i] + gamma * curr_val\n",
    "                parent.update_stats(curr_val)\n",
    "\n",
    "        # --- DEBUG TRACE--- Checking Prior vs MCTS dist\n",
    "        # print(f\"\\n--- MCTS Final Stats (Sims: {self.config['mcts_simulations']}) ---\")\n",
    "        \n",
    "        # # Sort children by visit count to see the winner clearly\n",
    "        # sorted_children = sorted(root.children.items(), key=lambda x: x[1].N, reverse=True)\n",
    "        \n",
    "        # for action_idx, child in sorted_children:\n",
    "        #     action_name = self.idx_to_action[action_idx]\n",
    "        #     # Print N (Visits), Q (Value), and P (Prior probability from NN)\n",
    "        #     print(f\"Action {action_name}: N={child.N:4d} | Q={child.Q:.4f} | Prior={child.P:.4f}\")\n",
    "            \n",
    "        if not root.children:\n",
    "            best_action_idx = self.NULL_ACTION_IDX\n",
    "        else:\n",
    "            best_action_idx = max(root.children, key=lambda idx: root.children[idx].N)\n",
    "        best_action = self.idx_to_action[best_action_idx]\n",
    "        \n",
    "        assignments = {}\n",
    "        srv_id, qid = best_action\n",
    "        if srv_id != \"NULL\":\n",
    "            srv = server_to_assign\n",
    "            if srv.server_id == srv_id:\n",
    "                assignments[srv] = net.stations[station_id_for_server].queues[qid]\n",
    "\n",
    "        return assignments, root, state_vec, root.Q, action_maskx\n",
    "\n",
    "    def _run_sim_step(self, sim_net: Network, action: Tuple[str, str]) -> Tuple[Network, float, Dict, bool, float]:\n",
    "        t_start = sim_net.t\n",
    "        start_areas = {st.station_id: sum(st._ql_area.values()) + st._sl_area for st in sim_net.stations.values()}\n",
    "        \n",
    "        action_policy = FixedActionPolicy(action)\n",
    "        sim_net.policy = action_policy\n",
    "        free_now = sim_net._get_free_servers()\n",
    "        if free_now:\n",
    "            assignments = action_policy.decide(sim_net, sim_net.t, free_now)\n",
    "            #print(\"3. assignments:\",assignments)\n",
    "            for srv, q in assignments.items():\n",
    "                if len(q) == 0 or srv.busy: continue\n",
    "                job = q.pop()\n",
    "                dep_time = srv.start_service(job, sim_net.t)\n",
    "                st_id = q.station_id\n",
    "                st = sim_net.stations[st_id]\n",
    "                idx = st.servers.index(srv)\n",
    "                #print(\"4. server_assigned\")\n",
    "                sim_net.schedule(dep_time, EventType.DEPARTURE, {\"station_id\": st_id, \"server_idx\": idx})\n",
    "        \n",
    "        (new_t, new_free) = sim_net.run_until_next_decision()\n",
    "        \n",
    "        dt = max(new_t - t_start, 1e-12)\n",
    "        total_sys_size_integral = 0.0\n",
    "        for st in sim_net.stations.values():\n",
    "            end_area = sum(st._ql_area.values()) + st._sl_area\n",
    "            total_sys_size_integral += (end_area - start_areas[st.station_id])\n",
    "            \n",
    "        avg_size = total_sys_size_integral / dt\n",
    "        step_reward = self._compute_score(avg_size)\n",
    "        is_terminal = (not sim_net._event_q) or (new_t > t_start + self.config[\"mcts_sim_horizon_s\"])\n",
    "        return sim_net, new_t, new_free, is_terminal, step_reward\n",
    "\n",
    "    def get_policy_target(self, root: MCTSNode, temperature: float) -> np.ndarray:\n",
    "        policy_target = np.zeros(len(self.master_action_list))\n",
    "        if not root.children:\n",
    "            policy_target[self.NULL_ACTION_IDX] = 1.0\n",
    "            return policy_target\n",
    "        visit_counts = np.array([child.N for child in root.children.values()])\n",
    "        action_indices = np.array([idx for idx in root.children.keys()])\n",
    "        \n",
    "        if temperature == 0:\n",
    "            best = action_indices[np.argmax(visit_counts)]\n",
    "            policy_target[best] = 1.0\n",
    "        else:\n",
    "            visit_counts = visit_counts ** (1.0 / temperature)\n",
    "            psum = np.sum(visit_counts)\n",
    "            if psum > 0:\n",
    "                probs = visit_counts / psum\n",
    "                for idx, prob in zip(action_indices, probs):\n",
    "                    policy_target[idx] = prob\n",
    "            else:\n",
    "                for idx in action_indices: policy_target[idx] = 1.0/len(action_indices)\n",
    "        return policy_target\n",
    "        \n",
    "class NN_Policy:    \n",
    "    def __init__(self, model: AlphaZeroNN):\n",
    "        config = CONFIG\n",
    "        self.model = model\n",
    "        self.config = CONFIG\n",
    "        self.device = config[\"device\"]\n",
    "        self.master_action_list = self._build_master_action_list()\n",
    "        self.action_to_idx = {action: i for i, action in enumerate(self.master_action_list)}\n",
    "        self.idx_to_action = {i: action for i, action in enumerate(self.master_action_list)}\n",
    "        self.NULL_ACTION_IDX = self.action_to_idx[(\"NULL\", \"NULL\")]\n",
    "\n",
    "    def _build_master_action_list(self) -> List[Tuple[str, str]]:\n",
    "        actions = []\n",
    "        for i in range(1, self.config[\"L\"] + 1):\n",
    "            station_id = f\"S{i}\"\n",
    "            server_id = f\"{station_id}-s0\"\n",
    "            for k in range(1, 4):\n",
    "                class_id = 3 * (i - 1) + k\n",
    "                queue_id = f\"Q{class_id}\"\n",
    "                actions.append((server_id, queue_id))\n",
    "        actions.append((\"NULL\", \"NULL\"))\n",
    "        return actions\n",
    "\n",
    "    def _get_server_context_idx(self, station_id: str) -> int:\n",
    "        # Assumes station_id is \"S1\", \"S2\", etc.\n",
    "        # Returns 0 for S1, 1 for S2...\n",
    "        return int(station_id.replace(\"S\", \"\")) - 1\n",
    "\n",
    "    \n",
    "    def _get_action_mask(self, sim_net: Network, free_servers: Dict[str, List[Server]]) -> np.ndarray:\n",
    "        mask = np.zeros(len(self.master_action_list), dtype=int)\n",
    "        \n",
    "        # 1. Identify which queues have jobs\n",
    "        non_empty_queues = {\n",
    "            (sid, qid) for sid, st in sim_net.stations.items()\n",
    "            for qid, q in st.queues.items() if len(q) > 0\n",
    "        }\n",
    "        \n",
    "        found_legal_action = False\n",
    "        \n",
    "        # 2. Find all valid server-to-queue assignments\n",
    "        for station_id, srvs in free_servers.items():\n",
    "            for srv in srvs:\n",
    "                for action_idx, (srv_id, qid) in enumerate(self.master_action_list):\n",
    "                    # STRICT MATCH: Ensure Action Server ID == Real Server ID\n",
    "                    if srv.server_id == srv_id:\n",
    "                        if (station_id, qid) in non_empty_queues:\n",
    "                            mask[action_idx] = 1\n",
    "                            found_legal_action = True\n",
    "                            \n",
    "        # 3. ONLY allow NULL if we found NO other legal actions\n",
    "        # This forces the policy to be \"Work Conserving\"\n",
    "        if not found_legal_action:\n",
    "            mask[self.NULL_ACTION_IDX] = 1\n",
    "            \n",
    "        return mask\n",
    "    \n",
    "    def _compute_score(self, mean_sys_size: float) -> float:\n",
    "        cat_val = self.config[\"CATASTROPHE_SOJOURN_TIME\"]\n",
    "        log_val = np.log1p(mean_sys_size)\n",
    "        ref_log = np.log1p(cat_val)\n",
    "        final_score = 1.0 - (2.0 * (log_val / ref_log))\n",
    "        return final_score\n",
    "        \n",
    "    def decide(self, net: Network, t: float, free_servers: Dict[str, List[Server]]) -> Dict[Server, Queue]:\n",
    "        assignments = {}\n",
    "        \n",
    "        # 1. Iterate over ALL stations with free servers\n",
    "        for st_id, srvs in free_servers.items():\n",
    "            for server in srvs:\n",
    "                # 2. Local Context for this specific server\n",
    "                # We define a \"Single Server\" context to force the NN to focus\n",
    "                context_free = {st_id: [server]}\n",
    "                \n",
    "                # Get mask for ONLY this server\n",
    "                # (This prevents the \"Ghost Assignment\" bug)\n",
    "                action_mask = self._get_action_mask(net, context_free)\n",
    "                \n",
    "                # 3. Setup Dummy Root & State\n",
    "                dummy_root = MCTSNode()\n",
    "                # Determine context index (S1->0, S2->1)\n",
    "                srv_ctx_idx = self._get_server_context_idx(st_id)\n",
    "                \n",
    "                # Check jobs - If station empty, skip this server\n",
    "                station = net.stations[st_id]\n",
    "                if sum(len(q) for q in station.queues.values()) == 0:\n",
    "                    continue\n",
    "                    \n",
    "                state_vec = dummy_root.get_state_vector(net, self.config[\"MAX_QUEUES_STATE\"], srv_ctx_idx)\n",
    "                \n",
    "                # 4. Neural Network Inference\n",
    "                state_tensor = torch.tensor(state_vec, dtype=torch.float32).unsqueeze(0).to(self.device)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    policy_logits, _ = self.model(state_tensor)\n",
    "                \n",
    "                policy_logits = policy_logits.squeeze(0)\n",
    "                \n",
    "                # 5. Apply Mask\n",
    "                mask_tensor = torch.tensor(action_mask, dtype=torch.bool).to(self.device)\n",
    "                policy_logits[~mask_tensor] = -1e9\n",
    "                \n",
    "                # 6. Select Action\n",
    "                best_action_idx = torch.argmax(policy_logits).item()\n",
    "                best_action = self.idx_to_action[best_action_idx]\n",
    "                \n",
    "                # 7. Decode Action\n",
    "                target_srv_id, target_q_id = best_action\n",
    "                \n",
    "                if target_srv_id != \"NULL\":\n",
    "                    # Double check we are assigning to the correct server ID\n",
    "                    if server.server_id == target_srv_id:\n",
    "                        assignments[server] = station.queues[target_q_id]\n",
    "\n",
    "        return assignments\n",
    "        \n",
    "class FixedActionPolicy(SchedulingPolicy):\n",
    "    def __init__(self, action): self.srv_id, self.qid = action\n",
    "    def decide(self, net, t, free_servers):\n",
    "        assignments = {}\n",
    "        if self.srv_id == \"NULL\":\n",
    "            return assignments\n",
    "        for st_id, srvs in free_servers.items():\n",
    "            for srv in srvs:\n",
    "                if srv.server_id == self.srv_id:\n",
    "                    assignments[srv] = net.stations[st_id].queues[self.qid]\n",
    "                    #print(\"2. matched!\")\n",
    "                    return assignments\n",
    "        #print(\"2. No match\", srv.server_id, self.srv_id)\n",
    "        return assignments\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, config: Dict[str, Any], model = None, start_states=None):\n",
    "        self.config = config\n",
    "        self.device = config[\"device\"]\n",
    "        self.seed = config[\"seed\"]\n",
    "        random.seed(self.seed)\n",
    "        np.random.seed(self.seed)\n",
    "        torch.manual_seed(self.seed)\n",
    "\n",
    "        temp_policy = MCTS_Policy(None, self.config)\n",
    "        self.action_space_size = len(temp_policy.master_action_list)\n",
    "        self.state_size = self.config[\"MAX_QUEUES_STATE\"] + self.config[\"L\"]\n",
    "        self.start_states = start_states\n",
    "        \n",
    "        if model is None:\n",
    "            self.model = AlphaZeroNN(self.state_size, self.action_space_size).to(self.device)\n",
    "        else: \n",
    "            self.model = model\n",
    "            \n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=config[\"learning_rate\"])\n",
    "        self.replay_buffer = ReplayBuffer(config[\"buffer_size\"])\n",
    "        self.mcts_policy = MCTS_Policy(self.model, self.config)\n",
    "        self.mcts_policy.model.eval()\n",
    "\n",
    "    def calculate_mean_system_size(self, net: Network) -> float:\n",
    "        total = sum(sum(st._ql_area.values()) + st._sl_area for st in net.stations.values())\n",
    "        return total / max(net.t, 1e-12)\n",
    "\n",
    "    def save_model(self, filepath=\"mcts_alphazero_model.pth\"):\n",
    "        torch.save(self.model.state_dict(), filepath)\n",
    "        print(f\"Model saved to {filepath}\")\n",
    "\n",
    "    def train_step(self) -> Optional[float]:\n",
    "        if len(self.replay_buffer) < self.config[\"batch_size\"]: return None \n",
    "        states, policy_targets, value_targets, action_mask = self.replay_buffer.sample(self.config[\"batch_size\"])\n",
    "        states = torch.tensor(states, dtype=torch.float32).to(self.device)\n",
    "        policy_targets = torch.tensor(policy_targets, dtype=torch.float32).to(self.device)\n",
    "        value_targets = torch.tensor(value_targets, dtype=torch.float32).to(self.device)\n",
    "        \n",
    "        self.model.train() \n",
    "        policy_logits, values = self.model(states)\n",
    "        \n",
    "        value_loss = F.mse_loss(values, value_targets)\n",
    "        action_mask = torch.tensor(action_mask, dtype=torch.bool).to(self.device)\n",
    "        policy_logits = policy_logits.masked_fill(action_mask == False, -torch.inf)\n",
    "        policy_log_probs = F.softmax(policy_logits, dim=1)\n",
    "        policy_loss = F.kl_div(policy_log_probs, policy_targets, reduction='batchmean')\n",
    "        total_loss = value_loss + policy_loss \n",
    "        self.optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return total_loss.item()\n",
    "\n",
    "    def run_training_loop_parallel(self, num_workers=4):\n",
    "        from mcts_worker import run_episode_worker \n",
    "        print(f\"\\n--- PARALLEL TRAINING: {self.config['num_train_loops']} loops x {self.config['episodes_per_loop']} eps ---\")\n",
    "        ctx = mp.get_context('spawn')\n",
    "        \n",
    "        for i in range(self.config[\"num_train_loops\"]):\n",
    "            print(f\"\\n--- LOOP {i+1}/{self.config['num_train_loops']} ---\")\n",
    "            cpu_model_state = {k: v.cpu() for k, v in self.model.state_dict().items()}\n",
    "            worker_args = []\n",
    "            for j in range(self.config[\"episodes_per_loop\"]):\n",
    "                seed = random.randint(0, 10000000)\n",
    "                snapshot = random.choice(self.start_states) if self.start_states else None\n",
    "                worker_args.append((j, cpu_model_state, self.config, seed, snapshot))\n",
    "            \n",
    "            episode_results = []\n",
    "            with ctx.Pool(processes=num_workers) as pool:\n",
    "                total_eps = self.config[\"episodes_per_loop\"]\n",
    "                completed_count = 0\n",
    "                results_iterator = pool.imap_unordered(run_episode_worker, worker_args)\n",
    "                \n",
    "                for history, score, raw_metric, duration in results_iterator:\n",
    "                    episode_results.append((history, score, raw_metric, duration))\n",
    "                    completed_count += 1\n",
    "                    print(f\"  > Episode {completed_count}/{total_eps} fin. (Score: {score:.3f}, Size: {raw_metric:.2f})\", end='\\r')\n",
    "            print(\"\") \n",
    "            \n",
    "            avg_score = 0\n",
    "            avg_raw = 0\n",
    "            total_samples = 0\n",
    "            \n",
    "            for history, score, raw_metric, duration in episode_results:\n",
    "                avg_score += score\n",
    "                avg_raw += raw_metric\n",
    "                for state_vec, policy_target, root_q, action_mask in history:\n",
    "                    self.replay_buffer.push(state_vec, policy_target, root_q, action_mask) #conflicted root_q or score.\n",
    "                    total_samples += 1\n",
    "                    \n",
    "            total_duration = sum(d for _, _, _, d in episode_results)\n",
    "            avg_duration = total_duration / len(episode_results)\n",
    "            \n",
    "            print(f\"  Generated {total_samples} samples.\")\n",
    "            print(f\"  Avg Score: {avg_score/len(episode_results):.4f}\")\n",
    "            print(f\"  Avg Sys Size: {avg_raw/len(episode_results):.4f}\")\n",
    "            print(f\"  Avg Episode Time: {avg_duration:.2f}s\") \n",
    "            print(f\"  Buffer Size: {len(self.replay_buffer)}\")\n",
    "            avg_loss = 0\n",
    "            t_steps = 0\n",
    "            for _ in range(self.config[\"train_steps_per_loop\"]):\n",
    "                loss = self.train_step()\n",
    "                if loss:\n",
    "                    avg_loss += loss\n",
    "                    t_steps += 1\n",
    "            if t_steps > 0: print(f\"  Avg Loss: {avg_loss/t_steps:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc497782-d817-4079-ba08-b87906e937ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_lbfs_start_states(num_states=200, warmup=5000.0, separation=500.0):\n",
    "    \"\"\"\n",
    "    Runs an LBFS simulation and captures snapshots of the system \n",
    "    at steady state to use as training starting points.\n",
    "    \"\"\"\n",
    "    print(f\"--- Generating {num_states} Warm Start States using LBFS ---\")\n",
    "    \n",
    "    # 1. Setup LBFS Environment\n",
    "    policy = LBFSPolicy()\n",
    "    net = ExtendedSixClassNetwork(policy=policy, L=2, seed=42)\n",
    "    \n",
    "    # 2. Warmup Phase (Get to steady state)\n",
    "    print(f\"Warming up for {warmup} time units...\")\n",
    "    net.run(until_time=warmup)\n",
    "    \n",
    "    start_states = []\n",
    "    \n",
    "    # 3. Sampling Phase\n",
    "    for i in range(num_states):\n",
    "        # Run for a bit to change the state (decorrelate samples)\n",
    "        run_until = net.t + separation\n",
    "        net.run(until_time=run_until)\n",
    "        \n",
    "        # Clone the state\n",
    "        # Note: We must clone it so we have a frozen copy\n",
    "        snapshot = net.clone()\n",
    "        start_states.append(snapshot)\n",
    "        \n",
    "        if (i+1) % 50 == 0:\n",
    "            print(f\"  Captured {i+1}/{num_states} states...\")\n",
    "            \n",
    "    print(\"Generation Complete.\")\n",
    "    return start_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "65b3cb07-8e96-4136-852f-bb9300988fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_policy = MCTS_Policy(None, CONFIG)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e7cdfe7-d2e1-4fdb-b6a7-eb6f670f993f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import heapq\n",
    "import torch.multiprocessing as mp\n",
    "from collections import deque, OrderedDict\n",
    "from typing import List, Dict, Tuple, Any, Optional\n",
    "from mcts_NetworkClasses import (\n",
    "    Network, \n",
    "    ExtendedSixClassNetwork,\n",
    "    SchedulingPolicy,\n",
    "    Server,\n",
    "    Queue,\n",
    "    EventType,\n",
    "    LBFSPolicy,\n",
    "    FIFONetPolicy\n",
    ")\n",
    "\n",
    "# --- Hyperparameters / Config ---\n",
    "CONFIG = {\n",
    "    # --- Network & NN API ---\n",
    "    \"L\": 2,                       # L=2 for ExtendedSixClassNetwork\n",
    "    \"MAX_QUEUES_STATE\": 10,       # Pad state vector to this size\n",
    "    \"MAX_STATIONS\": 2,            # Max stations (for building action list)\n",
    "    \"MAX_QUEUES_PER_STATION\": 3,  # Max queues per station (for action list)\n",
    "    \n",
    "    # --- NN & Training ---\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"buffer_size\": 50000,\n",
    "    \"batch_size\": 64,\n",
    "    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "\n",
    "    # --- MCTS ---\n",
    "    \"mcts_simulations\": 100,\n",
    "    \"c_puct\": 1.5,\n",
    "    \"mcts_sim_horizon_s\": 100.0,\n",
    "    \"temperature\": 1.0,\n",
    "    \"dirichlet_alpha\": 0.3,   # Alpha parameter for the noise\n",
    "    \"dirichlet_epsilon\": 0.25, # Weight of noise (25%) vs. policy (75%)\n",
    "    \n",
    "    # --- Training Loop ---\n",
    "    \"num_train_loops\": 100,\n",
    "    \"episodes_per_loop\": 50,      # Generate 50 sim runs\n",
    "    \"train_steps_per_loop\": 100,  # Train 100 times\n",
    "    \"sim_run_duration\": 5000.0,    # Run each \"real\" sim for Xs\n",
    "    \"CATASTROPHE_SOJOURN_TIME\": 80.0, # The \"worst\" score for normalization\n",
    "    \"seed\": 1\n",
    "}\n",
    "\n",
    "# --- 1. The Neural Network ---\n",
    "\n",
    "class AlphaZeroNN(nn.Module):\n",
    "    \"\"\"The 'Body-Head' neural network.\"\"\"\n",
    "    def __init__(self, state_size, action_space_size):\n",
    "        super(AlphaZeroNN, self).__init__()\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        self.action_space_size = action_space_size\n",
    "        \n",
    "        # Shared \"Body\"\n",
    "        self.body = nn.Sequential(\n",
    "            nn.Linear(self.state_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Policy Head (Predicts MCTS visit counts)\n",
    "        self.policy_head = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, self.action_space_size)\n",
    "        )\n",
    "        \n",
    "        # Value Head (Predicts game outcome)\n",
    "        self.value_head = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Tanh()  # Squashes value to [-1, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        body_out = self.body(state)\n",
    "        policy_logits = self.policy_head(body_out)\n",
    "        value = self.value_head(body_out)\n",
    "        return policy_logits, value\n",
    "\n",
    "# --- 2. The Replay Buffer ---\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Stores (state, policy_target, value_target) tuples.\"\"\"\n",
    "    def __init__(self, buffer_size):\n",
    "        self.buffer = deque(maxlen=buffer_size)\n",
    "\n",
    "    def push(self, state, policy_target, value_target):\n",
    "        self.buffer.append((state, policy_target, value_target))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, policy_targets, value_targets = zip(*batch)\n",
    "        \n",
    "        return (np.array(states), \n",
    "                np.array(policy_targets), \n",
    "                np.array(value_targets).reshape(-1, 1))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# --- 3. The MCTS Node ---\n",
    "\n",
    "class MCTSNode:\n",
    "    def __init__(self, parent: Optional[MCTSNode] = None, prior_p: float = 0.0):\n",
    "        self.parent = parent\n",
    "        self.children: Dict[int, MCTSNode] = {}  # action_idx -> Node\n",
    "        \n",
    "        self.N = 0      # Visit count\n",
    "        self.W = 0.0    # Total action value (sum of rewards)\n",
    "        self.Q = 0.0    # Mean action value (W / N)\n",
    "        self.P = prior_p  # Prior probability from NN\n",
    "        \n",
    "        self._state_vector: Optional[np.ndarray] = None\n",
    "        self._is_terminal: bool = False\n",
    "\n",
    "    def get_state_vector(self, sim_net: Network, max_queues: int) -> np.ndarray:\n",
    "        \"\"\"Gets the fixed-length state vector for the NN.\"\"\"\n",
    "        if self._state_vector is None:\n",
    "            q_lengths_dict = sim_net.total_queue_lengths()\n",
    "            # Sort by key: (S1, Q1), (S1, Q2), (S2, Q4)...\n",
    "            ordered_q_lengths = OrderedDict(sorted(q_lengths_dict.items()))\n",
    "            \n",
    "            state = [length for length in ordered_q_lengths.values()]\n",
    "            \n",
    "            # Pad with zeros\n",
    "            padding = [0] * (max_queues - len(state))\n",
    "            self._state_vector = np.array(state + padding)\n",
    "            \n",
    "            if len(self._state_vector) > max_queues:\n",
    "                self._state_vector = self._state_vector[:max_queues]\n",
    "                \n",
    "        return self._state_vector\n",
    "\n",
    "    def expand(self, policy_probs: np.ndarray):\n",
    "        \"\"\"Expand this node using NN policy priors.\"\"\"\n",
    "        for action_idx, prob in enumerate(policy_probs):\n",
    "            if prob > 0: # Only create nodes for legal actions\n",
    "                if action_idx not in self.children:\n",
    "                    self.children[action_idx] = MCTSNode(parent=self, prior_p=prob)\n",
    "\n",
    "    def select_child_puct(self, c_puct: float) -> Tuple[int, MCTSNode]:\n",
    "        \"\"\"Select the action/child that maximizes the PUCT score.\"\"\"\n",
    "        best_score = -np.inf\n",
    "        best_action_idx = -1\n",
    "        best_child = None\n",
    "\n",
    "        sqrt_self_N = math.sqrt(self.N)\n",
    "        \n",
    "        for action_idx, child in self.children.items():\n",
    "            score = child.Q + c_puct * child.P * (sqrt_self_N / (1 + child.N))\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_action_idx = action_idx\n",
    "                best_child = child\n",
    "        \n",
    "        return best_action_idx, best_child\n",
    "\n",
    "    def backpropagate(self, reward: float):\n",
    "        \"\"\"Update N and W values up the tree.\"\"\"\n",
    "        node = self\n",
    "        while node is not None:\n",
    "            node.N += 1\n",
    "            node.W += reward\n",
    "            node.Q = node.W / node.N\n",
    "            node = node.parent\n",
    "\n",
    "# --- 4. The MCTS Policy (The \"Planner\") ---\n",
    "\n",
    "class MCTS_Policy(SchedulingPolicy):\n",
    "    \"\"\"\n",
    "    This class is the MCTS planner. It plugs into the Network\n",
    "    as its \"policy\" object.\n",
    "    \"\"\"\n",
    "    def __init__(self, model: AlphaZeroNN, config: Dict[str, Any]):\n",
    "        self.model = model\n",
    "        self.config = config\n",
    "        self.device = config[\"device\"]\n",
    "        \n",
    "        self.master_action_list: List[Tuple[str, str]] = self._build_master_action_list()\n",
    "        self.action_to_idx: Dict[Tuple[str, str], int] = {\n",
    "            action: i for i, action in enumerate(self.master_action_list)\n",
    "        }\n",
    "        self.idx_to_action: Dict[int, Tuple[str, str]] = {\n",
    "            i: action for i, action in enumerate(self.master_action_list)\n",
    "        }\n",
    "        self.NULL_ACTION_IDX = self.action_to_idx[(\"NULL\", \"NULL\")]\n",
    "        \n",
    "        # for i, action in enumerate(self.master_action_list):\n",
    "        #     print(f\"  Idx {i}: {action}\")\n",
    "\n",
    "    def _build_master_action_list(self) -> List[Tuple[str, str]]:\n",
    "        actions = []\n",
    "        # Build based on L=2 network topology\n",
    "        for i in range(1, self.config[\"L\"] + 1):\n",
    "            station_id = f\"S{i}\"\n",
    "            server_id = f\"{station_id}-s0\" # Assumes 1 server 's0'\n",
    "            for k in range(1, 4):\n",
    "                class_id = 3 * (i - 1) + k\n",
    "                queue_id = f\"Q{class_id}\"\n",
    "                actions.append((server_id, queue_id))\n",
    "                \n",
    "        actions.append((\"NULL\", \"NULL\"))\n",
    "        return actions\n",
    "\n",
    "    def _get_action_mask(self, sim_net: Network, free_servers: Dict[str, List[Server]]) -> np.ndarray:\n",
    "        mask = np.zeros(len(self.master_action_list), dtype=int)\n",
    "        \n",
    "        non_empty_queues = {\n",
    "            (sid, qid) for sid, st in sim_net.stations.items()\n",
    "            for qid, q in st.queues.items() if len(q) > 0\n",
    "        }\n",
    "        \n",
    "        found_legal_action = False\n",
    "        for station_id, srvs in free_servers.items():\n",
    "            for srv in srvs:\n",
    "                for action_idx, (srv_id, qid) in enumerate(self.master_action_list):\n",
    "                    if srv.server_id == srv_id:\n",
    "                        if (station_id, qid) in non_empty_queues:\n",
    "                            mask[action_idx] = 1\n",
    "                            found_legal_action = True\n",
    "                            \n",
    "        if not found_legal_action and free_servers:\n",
    "            # If servers are free but no actions are possible,\n",
    "            # the *only* legal action is to do nothing.\n",
    "            mask[self.NULL_ACTION_IDX] = 1\n",
    "            \n",
    "        return mask\n",
    "\n",
    "    def decide(\n",
    "        self,\n",
    "        net: Network,\n",
    "        t: float,\n",
    "        free_servers: Dict[str, List[Server]],\n",
    "    ) -> Tuple[Dict[Server, Queue], MCTSNode, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Runs the full MCTS search and returns the best action,\n",
    "        the root node (for training data), and the state vector.\n",
    "        \"\"\"\n",
    "        \n",
    "        # --- 1. Sequential Assignment ---\n",
    "        # We only assign ONE server at a time.\n",
    "        # We find the *first* free server and make a decision for it.\n",
    "        # The Network will call us again if others are still free.\n",
    "        \n",
    "        server_to_assign: Optional[Server] = None\n",
    "        station_id_for_server: Optional[str] = None \n",
    "        for st_id, srvs in free_servers.items():\n",
    "            if srvs:\n",
    "                server_to_assign = srvs[0]\n",
    "                station_id_for_server = st_id     \n",
    "                break\n",
    "        \n",
    "        if server_to_assign is None:\n",
    "            # No free servers, return empty info\n",
    "            root = MCTSNode()\n",
    "            state_vec = root.get_state_vector(net, self.config[\"MAX_QUEUES_STATE\"])\n",
    "            return {}, root, state_vec\n",
    "        \n",
    "        search_free_servers = {station_id_for_server: [server_to_assign]}\n",
    "\n",
    "        # --- 2. Run MCTS Search ---\n",
    "        try:\n",
    "            real_snapshot = net.clone()\n",
    "        except Exception as e:\n",
    "            print(f\"CRITICAL ERROR: Failed to clone network: {e}\")\n",
    "            root = MCTSNode()\n",
    "            state_vec = root.get_state_vector(net, self.config[\"MAX_QUEUES_STATE\"])\n",
    "            return {}, root, state_vec # Fallback: do nothing\n",
    "\n",
    "        root = MCTSNode()\n",
    "        # Get the state vector for this root *before* search\n",
    "        state_vec = root.get_state_vector(net, self.config[\"MAX_QUEUES_STATE\"])\n",
    "        \n",
    "        for _ in range(self.config[\"mcts_simulations\"]):\n",
    "            node = root\n",
    "            sim_net = real_snapshot.clone()\n",
    "            sim_t = t\n",
    "            sim_free_servers = search_free_servers\n",
    "            \n",
    "            # --- SELECTION ---\n",
    "            path = [node]\n",
    "            while node.children: \n",
    "                action_idx, node = node.select_child_puct(self.config[\"c_puct\"])\n",
    "                path.append(node)\n",
    "                \n",
    "                (sim_net, sim_t, sim_free_servers, is_terminal) = self._run_sim_step(\n",
    "                    sim_net, \n",
    "                    self.idx_to_action[action_idx]\n",
    "                )\n",
    "                \n",
    "                if is_terminal:\n",
    "                    node._is_terminal = True\n",
    "                    break\n",
    "            \n",
    "            # --- EXPANSION & EVALUATION ---\n",
    "            value = 0.0 # Default terminal value\n",
    "            if not node._is_terminal:\n",
    "                leaf_state_vec = node.get_state_vector(sim_net, self.config[\"MAX_QUEUES_STATE\"])\n",
    "                state_tensor = torch.tensor(leaf_state_vec, dtype=torch.float32).unsqueeze(0).to(self.device)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    policy_logits, value_tensor = self.model(state_tensor)\n",
    "                \n",
    "                value = value_tensor.item()\n",
    "                action_mask = self._get_action_mask(sim_net, sim_free_servers)\n",
    "                \n",
    "                policy_logits = policy_logits.squeeze(0)\n",
    "                policy_logits[action_mask == 0] = -torch.inf\n",
    "                policy_probs = F.softmax(policy_logits, dim=0).cpu().numpy()\n",
    "                \n",
    "                node.expand(policy_probs)\n",
    "            \n",
    "            # --- BACKPROPAGATION ---\n",
    "            node.backpropagate(value)\n",
    "\n",
    "        # --- 3. Make Final Decision ---\n",
    "        if not root.children:\n",
    "            best_action_idx = self.NULL_ACTION_IDX\n",
    "        else:\n",
    "            # Select action based on visit counts\n",
    "            best_action_idx = max(\n",
    "                root.children, \n",
    "                key=lambda action_idx: root.children[action_idx].N\n",
    "            )\n",
    "        \n",
    "        best_action: Tuple[str, str] = self.idx_to_action[best_action_idx]\n",
    "        \n",
    "        # --- 4. Translate Action to Assignment ---\n",
    "        assignments: Dict[Server, Queue] = {}\n",
    "        (srv_id, qid) = best_action\n",
    "        \n",
    "        if srv_id != \"NULL\":\n",
    "            # Find the actual Server object\n",
    "            srv = server_to_assign\n",
    "            if srv.server_id == srv_id:\n",
    "                # <-- FIX 4: Use the stored station ID to find the queue\n",
    "                q = net.stations[station_id_for_server].queues[qid] \n",
    "                assignments[srv] = q\n",
    "            else:\n",
    "                # This is a logic error\n",
    "                print(f\"Warning: MCTS chose action for wrong server! {srv_id}\")\n",
    "\n",
    "        return assignments, root, state_vec\n",
    "\n",
    "    def _run_sim_step(\n",
    "        self, \n",
    "        sim_net: Network, \n",
    "        action: Tuple[str, str]\n",
    "    ) -> Tuple[Network, float, Dict[str, List[Server]], bool]:\n",
    "        \"\"\"Applies one MCTS action and runs sim to next decision.\"\"\"\n",
    "        \n",
    "        action_policy = FixedActionPolicy(action)\n",
    "        sim_net.policy = action_policy\n",
    "        \n",
    "        free_now = sim_net._get_free_servers()\n",
    "        if free_now:\n",
    "            assignments = action_policy.decide(sim_net, sim_net.t, free_now)\n",
    "            for srv, q in assignments.items():\n",
    "                if len(q) == 0 or srv.busy: continue\n",
    "                job = q.pop()\n",
    "                dep_time = srv.start_service(job, sim_net.t)\n",
    "                st_id = q.station_id\n",
    "                st = sim_net.stations[st_id]\n",
    "                server_idx = st.servers.index(srv)\n",
    "                sim_net.schedule(dep_time, EventType.DEPARTURE, {\"station_id\": st_id, \"server_idx\": server_idx})\n",
    "        \n",
    "        t_start = sim_net.t\n",
    "        (new_t, new_free_servers) = sim_net.run_until_next_decision()\n",
    "        \n",
    "        is_terminal = False\n",
    "        if not sim_net._event_q: # Check if event queue is empty\n",
    "            is_terminal = True\n",
    "        if new_t > t_start + self.config[\"mcts_sim_horizon_s\"]:\n",
    "            is_terminal = True\n",
    "            \n",
    "        return sim_net, new_t, new_free_servers, is_terminal\n",
    "    \n",
    "    ### --- MODIFIED (Made Public) --- ###\n",
    "    def get_policy_target(self, root: MCTSNode, temperature: float) -> np.ndarray:\n",
    "        \"\"\"Get the policy target (visit counts) to train the NN.\"\"\"\n",
    "        policy_target = np.zeros(len(self.master_action_list))\n",
    "        if not root.children:\n",
    "            policy_target[self.NULL_ACTION_IDX] = 1.0\n",
    "            return policy_target\n",
    "            \n",
    "        visit_counts = np.array([\n",
    "            child.N for child in root.children.values()\n",
    "        ])\n",
    "        action_indices = np.array([\n",
    "            action_idx for action_idx in root.children.keys()\n",
    "        ])\n",
    "        \n",
    "        if temperature == 0:\n",
    "            best_action_local_idx = np.argmax(visit_counts)\n",
    "            best_action_global_idx = action_indices[best_action_local_idx]\n",
    "            policy_target[best_action_global_idx] = 1.0\n",
    "        else:\n",
    "            visit_counts_temp = visit_counts ** (1.0 / temperature)\n",
    "            probs_sum = np.sum(visit_counts_temp)\n",
    "            if probs_sum == 0:\n",
    "                # All counts are 0, use uniform\n",
    "                for idx in action_indices:\n",
    "                    policy_target[idx] = 1.0 / len(action_indices)\n",
    "            else:\n",
    "                probs = visit_counts_temp / probs_sum\n",
    "                for idx, prob in zip(action_indices, probs):\n",
    "                    policy_target[idx] = prob\n",
    "                \n",
    "        return policy_target\n",
    "\n",
    "# --- 5. Helper Policy for MCTS ---\n",
    "\n",
    "class FixedActionPolicy(SchedulingPolicy):\n",
    "    \"\"\"A dummy policy that executes one pre-selected action.\"\"\"\n",
    "    def __init__(self, action: Tuple[str, str]):\n",
    "        self.srv_id, self.qid = action\n",
    "\n",
    "    def decide(\n",
    "        self, \n",
    "        net: Network, \n",
    "        t: float, \n",
    "        free_servers: Dict[str, List[Server]]\n",
    "    ) -> Dict[Server, Queue]:\n",
    "        \n",
    "        assignments = {}\n",
    "        if self.srv_id == \"NULL\":\n",
    "            return assignments\n",
    "            \n",
    "        for st_id, srvs in free_servers.items():\n",
    "            for srv in srvs:\n",
    "                if srv.server_id == self.srv_id:\n",
    "                    q = net.stations[st_id].queues[self.qid]\n",
    "                    assignments[srv] = q\n",
    "                    return assignments\n",
    "        return assignments\n",
    "\n",
    "class Trainer:\n",
    "    \"\"\"The orchestrator that manages training.\"\"\"\n",
    "    def __init__(self, config: Dict[str, Any], model = None, start_states=None):\n",
    "        self.config = config\n",
    "        self.device = config[\"device\"]\n",
    "        self.seed = config[\"seed\"]\n",
    "        random.seed(self.seed)\n",
    "        np.random.seed(self.seed)\n",
    "        torch.manual_seed(self.seed)\n",
    "\n",
    "        temp_policy = MCTS_Policy(None, self.config) # Dummy to get sizes\n",
    "        self.action_space_size = len(temp_policy.master_action_list)\n",
    "        self.state_size = self.config[\"MAX_QUEUES_STATE\"]\n",
    "        self.start_states = start_states\n",
    "        \n",
    "        print(f\"Trainer init: state_size={self.state_size}, action_space_size={self.action_space_size}\")\n",
    "\n",
    "        if model == None:\n",
    "            self.model = AlphaZeroNN(\n",
    "                self.state_size, \n",
    "                self.action_space_size\n",
    "            ).to(self.device)\n",
    "            \n",
    "            self.optimizer = optim.Adam(\n",
    "                self.model.parameters(), \n",
    "                lr=config[\"learning_rate\"]\n",
    "            )\n",
    "        else: \n",
    "            self.model = model\n",
    "        self.replay_buffer = ReplayBuffer(config[\"buffer_size\"])\n",
    "        \n",
    "        self.mcts_policy = MCTS_Policy(self.model, self.config)\n",
    "        self.mcts_policy.model.eval()\n",
    "\n",
    "    def run_simulation_episode(self) -> float:\n",
    "        \"\"\"\n",
    "        Simulates one full episode by running its *own* event loop\n",
    "        to capture (state, policy) data at each decision.\n",
    "        \"\"\"\n",
    "        if self.start_states:\n",
    "            random_snapshot = random.choice(self.start_states)\n",
    "            net = random_snapshot.clone()\n",
    "            net.policy = self.mcts_policy\n",
    "            \n",
    "            # --- CRITICAL FIX: RESET STATS ---\n",
    "            # We want to measure performance ONLY for this 500s window.\n",
    "            # We must wipe the \"LBFS History\" from the metrics variables.\n",
    "            \n",
    "            # 1. Reset Time to 0 (so we measure duration relative to now)\n",
    "            net.t = 0.0 \n",
    "            \n",
    "            # 2. Reset Areas to 0\n",
    "            for st in net.stations.values():\n",
    "                st._ql_area = {qid: 0.0 for qid in st.queues}\n",
    "                st._sl_area = 0.0\n",
    "            \n",
    "            # 3. Reset Counters\n",
    "            net.completed_jobs = 0\n",
    "            net.sum_sojourn = 0.0\n",
    "            # ---------------------------------\n",
    "            \n",
    "        else:\n",
    "        # 1. Create a new \"real\" simulation\n",
    "            net = ExtendedSixClassNetwork(\n",
    "                policy=self.mcts_policy, # The MCTS policy\n",
    "                L=self.config[\"L\"],\n",
    "                seed=random.randint(0, 1_000_000)\n",
    "            )\n",
    "            \n",
    "        episode_history: List[Tuple[np.ndarray, np.ndarray]] = []\n",
    "        \n",
    "        # 2. Seed the simulation (from Network.run())\n",
    "        if not net._seeded:\n",
    "            for ap in net.arrivals:\n",
    "                t_next = ap.schedule_next(net.t)\n",
    "                net.schedule(t_next, EventType.ARRIVAL, {\"ap\": ap})\n",
    "            net._seeded = True\n",
    "        \n",
    "        # 3. Run the \"Outer\" simulation event loop\n",
    "        while net._event_q:\n",
    "            if net._event_q[0].time > self.config[\"sim_run_duration\"]:\n",
    "                break\n",
    "    \n",
    "            ev = heapq.heappop(net._event_q)\n",
    "    \n",
    "            # --- This logic is copied from Network.run() ---\n",
    "            dt = ev.time - net.t\n",
    "            if dt > 0:\n",
    "                for st in net.stations.values():\n",
    "                    for qid, q in st.queues.items():\n",
    "                        st._ql_area[qid] += len(q) * dt\n",
    "                    num_busy_servers = sum(1 for srv in st.servers if srv.busy)\n",
    "                    st._sl_area += num_busy_servers * dt\n",
    "            net.t = ev.time\n",
    "            # --- End copied logic ---\n",
    "    \n",
    "            # Handle event\n",
    "            if ev.type == EventType.ARRIVAL:\n",
    "                net._on_arrival(ev.payload[\"ap\"])\n",
    "            elif ev.type == EventType.DEPARTURE:\n",
    "                net._on_departure(ev.payload[\"station_id\"], ev.payload[\"server_idx\"])\n",
    "    \n",
    "            # --- DATA CAPTURE HOOK ---\n",
    "            # This is the \"Scheduling decision\" part\n",
    "            free_servers = net._get_free_servers()\n",
    "            \n",
    "            while free_servers:\n",
    "                # 1. A decision is needed. Call MCTS.\n",
    "                assignments, root, state_vec = self.mcts_policy.decide(\n",
    "                    net, net.t, free_servers\n",
    "                )\n",
    "                \n",
    "                # 2. Get the training data\n",
    "                policy_target = self.mcts_policy.get_policy_target(\n",
    "                    root, self.config[\"temperature\"]\n",
    "                )\n",
    "                \n",
    "                # 3. Store for later\n",
    "                episode_history.append((state_vec, policy_target))\n",
    "                \n",
    "                if not assignments:\n",
    "                    # MCTS returned \"NULL\" action\n",
    "                    break # Exit the 'while free_servers' loop\n",
    "                \n",
    "                # 4. Apply the *single* assignment\n",
    "                for srv, q in assignments.items():\n",
    "                    if len(q) == 0 or srv.busy: continue\n",
    "                    job = q.pop()\n",
    "                    dep_time = srv.start_service(job, net.t)\n",
    "                    st_id = q.station_id\n",
    "                    st = net.stations[st_id]\n",
    "                    server_idx = st.servers.index(srv)\n",
    "                    net.schedule(dep_time, EventType.DEPARTURE, {\"station_id\": st_id, \"server_idx\": server_idx})\n",
    "\n",
    "                # 5. Check for more free servers *immediately*\n",
    "                free_servers = net._get_free_servers()\n",
    "                # Loop continues until all servers are assigned\n",
    "        \n",
    "        # 4. Episode is over, get final outcome (z)\n",
    "        final_outcome_z, mean_sojourn = self.get_final_outcome(net)\n",
    "        \n",
    "        # 5. Add all steps to replay buffer with the final outcome\n",
    "        if not episode_history:\n",
    "            print(\"Warning: Episode ended with 0 decisions made.\")\n",
    "            return 0.0\n",
    "\n",
    "        for state, policy_target in episode_history:\n",
    "            self.replay_buffer.push(state, policy_target, final_outcome_z)\n",
    "            \n",
    "        return final_outcome_z, mean_sojourn\n",
    "  # --- Updated Reward Function (For Training) ---\n",
    "    # Use to optimize for System Size specifically\n",
    "    def get_final_outcome(self, net: Network) -> Tuple[float, float]:\n",
    "        \"\"\"\n",
    "        Returns (normalized_score, raw_metric).\n",
    "        Metric is MEAN SYSTEM SIZE.\n",
    "        \"\"\"\n",
    "        # 1. Calculate raw metric\n",
    "        mean_sys_size = calculate_mean_system_size(net)\n",
    "        \n",
    "        # 2. Normalize\n",
    "        # Note: You might need to adjust CATASTROPHE_SOJOURN_TIME in config \n",
    "        # to be a reasonable 'max system size' (e.g., 50 or 100 jobs)\n",
    "        catastrophe_val = self.config[\"CATASTROPHE_SOJOURN_TIME\"] \n",
    "        \n",
    "        clipped_val = np.clip(mean_sys_size, 0, catastrophe_val)\n",
    "        scaled_val = clipped_val / catastrophe_val\n",
    "        final_score = 1.0 - (2.0 * scaled_val)\n",
    "        \n",
    "        return float(final_score), mean_sys_size\n",
    "\n",
    "    def train_step(self) -> Optional[float]:\n",
    "        \"\"\"Samples a batch and performs one backprop step.\"\"\"\n",
    "        if len(self.replay_buffer) < self.config[\"batch_size\"]:\n",
    "            return None \n",
    "\n",
    "        states, policy_targets, value_targets = self.replay_buffer.sample(\n",
    "            self.config[\"batch_size\"]\n",
    "        )\n",
    "        \n",
    "        states = torch.tensor(states, dtype=torch.float32).to(self.device)\n",
    "        policy_targets = torch.tensor(policy_targets, dtype=torch.float32).to(self.device)\n",
    "        value_targets = torch.tensor(value_targets, dtype=torch.float32).to(self.device)\n",
    "        \n",
    "        self.model.train() \n",
    "        policy_logits, values = self.model(states)\n",
    "        \n",
    "        value_loss = F.mse_loss(values, value_targets)\n",
    "        policy_loss = F.cross_entropy(policy_logits, policy_targets)\n",
    "        total_loss = value_loss + policy_loss # weight should not be equal\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return total_loss.item()\n",
    "\n",
    "    def run_training_loop_parallel(self, num_workers=4):\n",
    "        \"\"\"\n",
    "        Runs the training loop using Multiprocessing.\n",
    "        \"\"\"\n",
    "        # Import inside function to avoid circular dependency\n",
    "        from mcts_worker import run_episode_worker \n",
    "\n",
    "        print(f\"\\n--- PARALLEL TRAINING: {self.config['num_train_loops']} loops x {self.config['episodes_per_loop']} episodes ---\")\n",
    "        print(f\"Workers: {num_workers}\")\n",
    "        \n",
    "        ctx = mp.get_context('spawn')\n",
    "        \n",
    "        for i in range(self.config[\"num_train_loops\"]):\n",
    "            print(f\"\\n--- LOOP {i+1}/{self.config['num_train_loops']} ---\")\n",
    "            \n",
    "            # 1. Prepare Data\n",
    "            cpu_model_state = {k: v.cpu() for k, v in self.model.state_dict().items()}\n",
    "            \n",
    "            worker_args = []\n",
    "            for j in range(self.config[\"episodes_per_loop\"]):\n",
    "                seed = random.randint(0, 10000000)\n",
    "                snapshot = random.choice(self.start_states) if self.start_states else None\n",
    "                # Add worker ID 'j' for logging\n",
    "                worker_args.append((j, cpu_model_state, self.config, seed, snapshot))\n",
    "            \n",
    "            # 2. Run Parallel Generation (UPDATED FOR LIVE PROGRESS)\n",
    "            episode_results = []\n",
    "            \n",
    "            with ctx.Pool(processes=num_workers) as pool:\n",
    "                # Use imap_unordered to process results AS THEY ARRIVE\n",
    "                total_eps = self.config[\"episodes_per_loop\"]\n",
    "                completed_count = 0\n",
    "                \n",
    "                # Create iterator\n",
    "                results_iterator = pool.imap_unordered(run_episode_worker, worker_args)\n",
    "                \n",
    "                for history, score, raw_metric in results_iterator:\n",
    "                    episode_results.append((history, score, raw_metric))\n",
    "                    completed_count += 1\n",
    "                    \n",
    "                    # Print progress bar on the SAME LINE (\\r)\n",
    "                    print(f\"  > Episode {completed_count}/{total_eps} finished. (Last Score: {score:.3f}, Size: {raw_metric:.2f})\", end='\\r')\n",
    "            \n",
    "            # Print new line after loop finishes so we don't overwrite the progress bar\n",
    "            print(\"\") \n",
    "            \n",
    "            # 3. Process Results (Same as before)\n",
    "            avg_score = 0\n",
    "            avg_raw = 0\n",
    "            total_new_samples = 0\n",
    "            \n",
    "            for history, score, raw_metric in episode_results:\n",
    "                avg_score += score\n",
    "                avg_raw += raw_metric\n",
    "                \n",
    "                for state_vec, policy_target in history:\n",
    "                    self.replay_buffer.push(state_vec, policy_target, score)\n",
    "                    total_new_samples += 1\n",
    "            \n",
    "            print(f\"  Generated {total_new_samples} samples.\")\n",
    "            print(f\"  Avg Score: {avg_score / len(episode_results):.4f}\")\n",
    "            print(f\"  Avg Sys Size: {avg_raw / len(episode_results):.4f}\")\n",
    "            \n",
    "            # 4. Train\n",
    "            avg_loss = 0\n",
    "            train_steps = 0\n",
    "            for _ in range(self.config[\"train_steps_per_loop\"]):\n",
    "                loss = self.train_step()\n",
    "                if loss:\n",
    "                    avg_loss += loss\n",
    "                    train_steps += 1\n",
    "            \n",
    "            if train_steps > 0:\n",
    "                print(f\"  Avg Loss: {avg_loss / train_steps:.4f}\")\n",
    "            \n",
    "            if (i+1) % 10 == 0:\n",
    "                self.save_model(f\"model_loop_{i+1}.pth\")\n",
    "    \n",
    "       # --- Helper to calculate System Size from a Network object ---\n",
    "    def calculate_mean_system_size(self, net: Network) -> float:\n",
    "        \"\"\"\n",
    "        Calculates the time-averaged number of jobs in the system.\n",
    "        Formula: (Integral of Q(t) + S(t)) / Total Time\n",
    "        \"\"\"\n",
    "        total_area = 0.0\n",
    "        \n",
    "        # Sum area of all queues and servers across all stations\n",
    "        for st in net.stations.values():\n",
    "            # Add Queue Area (waiting jobs * time)\n",
    "            total_area += sum(st._ql_area.values())\n",
    "            # Add Service Area (busy servers * time)\n",
    "            total_area += st._sl_area\n",
    "            \n",
    "        elapsed_time = max(net.t, 1e-12) # Avoid division by zero\n",
    "        return total_area / elapsed_time\n",
    "\n",
    "    def save_model(self, filepath=\"mcts_alphazero_model.pth\"):\n",
    "        \"\"\"Saves the trained model weights.\"\"\"\n",
    "        torch.save(self.model.state_dict(), filepath)\n",
    "        print(f\"Model saved to {filepath}\")\n",
    "    \n",
    "    # --- Updated Evaluation Method ---\n",
    "    def evaluate_final_policy(self, num_episodes=10) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Runs evaluative episodes and reports MEAN SYSTEM SIZE (L).\n",
    "        \"\"\"\n",
    "        print(f\"\\n--- Starting Final Evaluation (Metric: System Size, {num_episodes} episodes) ---\")\n",
    "        \n",
    "        # 1. Disable Exploration for now\n",
    "        old_temp = self.config[\"temperature\"]\n",
    "        old_eps = self.config[\"dirichlet_epsilon\"]\n",
    "        self.config[\"temperature\"] = 0.0 \n",
    "        self.config[\"dirichlet_epsilon\"] = 0.0\n",
    "        \n",
    "        self.mcts_policy.model.eval()\n",
    "        \n",
    "        system_sizes = []\n",
    "        total_completed = 0\n",
    "        \n",
    "        for i in range(num_episodes):\n",
    "            net = ExtendedSixClassNetwork(\n",
    "                policy=self.mcts_policy,\n",
    "                L=self.config[\"L\"],\n",
    "                seed=random.randint(100000, 999999)\n",
    "            )\n",
    "            \n",
    "            if not net._seeded:\n",
    "                for ap in net.arrivals:\n",
    "                    t_next = ap.schedule_next(net.t)\n",
    "                    net.schedule(t_next, EventType.ARRIVAL, {\"ap\": ap})\n",
    "                net._seeded = True\n",
    "            \n",
    "            \n",
    "            while net._event_q:\n",
    "                if net._event_q[0].time > self.config[\"sim_run_duration\"]:\n",
    "                    break\n",
    "\n",
    "                print(net.t)\n",
    "                ev = heapq.heappop(net._event_q)\n",
    "                \n",
    "                dt = ev.time - net.t\n",
    "                if dt > 0:\n",
    "                    for st in net.stations.values():\n",
    "                        for qid, q in st.queues.items():\n",
    "                            st._ql_area[qid] += len(q) * dt\n",
    "                        num_busy = sum(1 for srv in st.servers if srv.busy)\n",
    "                        st._sl_area += num_busy * dt\n",
    "                net.t = ev.time\n",
    "    \n",
    "                if ev.type == EventType.ARRIVAL:\n",
    "                    net._on_arrival(ev.payload[\"ap\"])\n",
    "                elif ev.type == EventType.DEPARTURE:\n",
    "                    net._on_departure(ev.payload[\"station_id\"], ev.payload[\"server_idx\"])\n",
    "    \n",
    "                free_servers = net._get_free_servers()\n",
    "                while free_servers:\n",
    "                    assignments, _, _ = self.mcts_policy.decide(net, net.t, free_servers)\n",
    "                    if not assignments: break\n",
    "                    for srv, q in assignments.items():\n",
    "                        if len(q) == 0 or srv.busy: continue\n",
    "                        job = q.pop()\n",
    "                        srv.start_service(job, net.t)\n",
    "                        st_id = q.station_id\n",
    "                        server_idx = net.stations[st_id].servers.index(srv)\n",
    "                        net.schedule(net.t + srv.service_sampler(job), EventType.DEPARTURE, {\"station_id\": st_id, \"server_idx\": server_idx})\n",
    "                    free_servers = net._get_free_servers()\n",
    "\n",
    "                if net.t % 2000 == 0:\n",
    "                    print(net.t)\n",
    "                    print(self.calculate_mean_system_size(net))\n",
    "            \n",
    "            # --- Calculate System Size ---\n",
    "            mean_sys_size = self.calculate_mean_system_size(net)\n",
    "            system_sizes.append(mean_sys_size)\n",
    "            \n",
    "            total_completed += net.completed_jobs\n",
    "            print(f\"  Eval Episode {i+1}: Mean System Size = {mean_sys_size:.4f}, Jobs = {net.completed_jobs}\")\n",
    "    \n",
    "        # 2. Restore Config\n",
    "        self.config[\"temperature\"] = old_temp\n",
    "        self.config[\"dirichlet_epsilon\"] = old_eps\n",
    "        \n",
    "        avg_sys_size = np.mean(system_sizes)\n",
    "        \n",
    "        results = {\n",
    "            \"mean_system_size\": avg_sys_size,\n",
    "            \"std_system_size\": np.std(system_sizes),\n",
    "            \"total_jobs\": total_completed\n",
    "        }\n",
    "        \n",
    "        print(\"\\n--- Evaluation Results ---\")\n",
    "        print(f\"Average Mean System Size: {avg_sys_size:.4f}\")\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc497782-d817-4079-ba08-b87906e937ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_lbfs_start_states(num_states=200, warmup=5000.0, separation=500.0):\n",
    "    \"\"\"\n",
    "    Runs an LBFS simulation and captures snapshots of the system \n",
    "    at steady state to use as training starting points.\n",
    "    \"\"\"\n",
    "    print(f\"--- Generating {num_states} Warm Start States using LBFS ---\")\n",
    "    \n",
    "    # 1. Setup LBFS Environment\n",
    "    policy = LBFSPolicy()\n",
    "    net = ExtendedSixClassNetwork(policy=policy, L=2, seed=42)\n",
    "    \n",
    "    # 2. Warmup Phase (Get to steady state)\n",
    "    print(f\"Warming up for {warmup} time units...\")\n",
    "    net.run(until_time=warmup)\n",
    "    \n",
    "    start_states = []\n",
    "    \n",
    "    # 3. Sampling Phase\n",
    "    for i in range(num_states):\n",
    "        # Run for a bit to change the state (decorrelate samples)\n",
    "        run_until = net.t + separation\n",
    "        net.run(until_time=run_until)\n",
    "        \n",
    "        # Clone the state\n",
    "        # Note: We must clone it so we have a frozen copy\n",
    "        snapshot = net.clone()\n",
    "        start_states.append(snapshot)\n",
    "        \n",
    "        if (i+1) % 50 == 0:\n",
    "            print(f\"  Captured {i+1}/{num_states} states...\")\n",
    "            \n",
    "    print(\"Generation Complete.\")\n",
    "    return start_states\n",
    "\n",
    "# Generate them (this takes a few seconds/minutes)\n",
    "# 200 states is usually enough; we will pick randomly from them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3214a4e-99d7-46ac-b430-3b37d8802516",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "027bc9e1-3400-4d95-80df-0fb4dd0f4f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import heapq\n",
    "from collections import deque, OrderedDict\n",
    "from typing import List, Dict, Tuple, Any, Optional\n",
    "from mcts_NetworkClasses import (\n",
    "    Network, \n",
    "    ExtendedSixClassNetwork,\n",
    "    SchedulingPolicy,\n",
    "    Server,\n",
    "    Queue,\n",
    "    EventType\n",
    ")\n",
    "\n",
    "# --- Hyperparameters / Config ---\n",
    "CONFIG = {\n",
    "    # --- Network & NN API ---\n",
    "    \"L\": 2,                       # L=2 for ExtendedSixClassNetwork\n",
    "    \"MAX_QUEUES_STATE\": 10,       # Pad state vector to this size\n",
    "    \"MAX_STATIONS\": 2,            # Max stations (for building action list)\n",
    "    \"MAX_QUEUES_PER_STATION\": 3,  # Max queues per station (for action list)\n",
    "    \n",
    "    # --- NN & Training ---\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"buffer_size\": 50000,\n",
    "    \"batch_size\": 64,\n",
    "    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "\n",
    "    # --- MCTS ---\n",
    "    \"mcts_simulations\": 100,\n",
    "    \"c_puct\": 1.5,\n",
    "    \"mcts_sim_horizon_s\": 100.0,\n",
    "    \"temperature\": 1.0,\n",
    "    \"dirichlet_alpha\": 0.3,   # Alpha parameter for the noise\n",
    "    \"dirichlet_epsilon\": 0.25, # Weight of noise (25%) vs. policy (75%)\n",
    "    \n",
    "    # --- Training Loop ---\n",
    "    \"num_train_loops\": 100,\n",
    "    \"episodes_per_loop\": 50,      # Generate 50 sim runs\n",
    "    \"train_steps_per_loop\": 100,  # Train 100 times\n",
    "    \"sim_run_duration\": 500.0,    # Run each \"real\" sim for 500s\n",
    "    \"CATASTROPHE_SOJOURN_TIME\": 200.0, # The \"worst\" score for normalization\n",
    "    \"seed\": 1\n",
    "}\n",
    "\n",
    "# --- 1. The Neural Network ---\n",
    "\n",
    "class AlphaZeroNN(nn.Module):\n",
    "    \"\"\"The 'Body-Head' neural network.\"\"\"\n",
    "    def __init__(self, state_size, action_space_size):\n",
    "        super(AlphaZeroNN, self).__init__()\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        self.action_space_size = action_space_size\n",
    "        \n",
    "        # Shared \"Body\"\n",
    "        self.body = nn.Sequential(\n",
    "            nn.Linear(self.state_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Policy Head (Predicts MCTS visit counts)\n",
    "        self.policy_head = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, self.action_space_size)\n",
    "        )\n",
    "        \n",
    "        # Value Head (Predicts game outcome)\n",
    "        self.value_head = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Tanh()  # Squashes value to [-1, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        body_out = self.body(state)\n",
    "        policy_logits = self.policy_head(body_out)\n",
    "        value = self.value_head(body_out)\n",
    "        return policy_logits, value\n",
    "\n",
    "# --- 2. The Replay Buffer ---\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Stores (state, policy_target, value_target) tuples.\"\"\"\n",
    "    def __init__(self, buffer_size):\n",
    "        self.buffer = deque(maxlen=buffer_size)\n",
    "\n",
    "    def push(self, state, policy_target, value_target):\n",
    "        self.buffer.append((state, policy_target, value_target))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, policy_targets, value_targets = zip(*batch)\n",
    "        \n",
    "        return (np.array(states), \n",
    "                np.array(policy_targets), \n",
    "                np.array(value_targets).reshape(-1, 1))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# --- 3. The MCTS Node ---\n",
    "\n",
    "class MCTSNode:\n",
    "    def __init__(self, parent: Optional[MCTSNode] = None, prior_p: float = 0.0):\n",
    "        self.parent = parent\n",
    "        self.children: Dict[int, MCTSNode] = {}  # action_idx -> Node\n",
    "        \n",
    "        self.N = 0      # Visit count\n",
    "        self.W = 0.0    # Total action value (sum of rewards)\n",
    "        self.Q = 0.0    # Mean action value (W / N)\n",
    "        self.P = prior_p  # Prior probability from NN\n",
    "        \n",
    "        self._state_vector: Optional[np.ndarray] = None\n",
    "        self._is_terminal: bool = False\n",
    "\n",
    "    def get_state_vector(self, sim_net: Network, max_queues: int) -> np.ndarray:\n",
    "        \"\"\"Gets the fixed-length state vector for the NN.\"\"\"\n",
    "        if self._state_vector is None:\n",
    "            q_lengths_dict = sim_net.total_queue_lengths()\n",
    "            # Sort by key: (S1, Q1), (S1, Q2), (S2, Q4)...\n",
    "            ordered_q_lengths = OrderedDict(sorted(q_lengths_dict.items()))\n",
    "            \n",
    "            state = [length for length in ordered_q_lengths.values()]\n",
    "            \n",
    "            # Pad with zeros\n",
    "            padding = [0] * (max_queues - len(state))\n",
    "            self._state_vector = np.array(state + padding)\n",
    "            \n",
    "            if len(self._state_vector) > max_queues:\n",
    "                self._state_vector = self._state_vector[:max_queues]\n",
    "                \n",
    "        return self._state_vector\n",
    "\n",
    "    def expand(self, policy_probs: np.ndarray):\n",
    "        \"\"\"Expand this node using NN policy priors.\"\"\"\n",
    "        for action_idx, prob in enumerate(policy_probs):\n",
    "            if prob > 0: # Only create nodes for legal actions\n",
    "                if action_idx not in self.children:\n",
    "                    self.children[action_idx] = MCTSNode(parent=self, prior_p=prob)\n",
    "\n",
    "    def select_child_puct(self, c_puct: float) -> Tuple[int, MCTSNode]:\n",
    "        \"\"\"Select the action/child that maximizes the PUCT score.\"\"\"\n",
    "        best_score = -np.inf\n",
    "        best_action_idx = -1\n",
    "        best_child = None\n",
    "\n",
    "        sqrt_self_N = math.sqrt(self.N)\n",
    "        \n",
    "        for action_idx, child in self.children.items():\n",
    "            score = child.Q + c_puct * child.P * (sqrt_self_N / (1 + child.N))\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_action_idx = action_idx\n",
    "                best_child = child\n",
    "        \n",
    "        return best_action_idx, best_child\n",
    "\n",
    "    def backpropagate(self, reward: float):\n",
    "        \"\"\"Update N and W values up the tree.\"\"\"\n",
    "        node = self\n",
    "        while node is not None:\n",
    "            node.N += 1\n",
    "            node.W += reward\n",
    "            node.Q = node.W / node.N\n",
    "            node = node.parent\n",
    "\n",
    "# --- 4. The MCTS Policy (The \"Planner\") ---\n",
    "\n",
    "class MCTS_Policy(SchedulingPolicy):\n",
    "    \"\"\"\n",
    "    This class is the MCTS planner. It plugs into the Network\n",
    "    as its \"policy\" object.\n",
    "    \"\"\"\n",
    "    def __init__(self, model: AlphaZeroNN, config: Dict[str, Any]):\n",
    "        self.model = model\n",
    "        self.config = config\n",
    "        self.device = config[\"device\"]\n",
    "        \n",
    "        self.master_action_list: List[Tuple[str, str]] = self._build_master_action_list()\n",
    "        self.action_to_idx: Dict[Tuple[str, str], int] = {\n",
    "            action: i for i, action in enumerate(self.master_action_list)\n",
    "        }\n",
    "        self.idx_to_action: Dict[int, Tuple[str, str]] = {\n",
    "            i: action for i, action in enumerate(self.master_action_list)\n",
    "        }\n",
    "        self.NULL_ACTION_IDX = self.action_to_idx[(\"NULL\", \"NULL\")]\n",
    "        \n",
    "        print(f\"MCTS Policy initialized with {len(self.master_action_list)} master actions.\")\n",
    "        # for i, action in enumerate(self.master_action_list):\n",
    "        #     print(f\"  Idx {i}: {action}\")\n",
    "\n",
    "    def _build_master_action_list(self) -> List[Tuple[str, str]]:\n",
    "        actions = []\n",
    "        # Build based on L=2 network topology\n",
    "        for i in range(1, self.config[\"L\"] + 1):\n",
    "            station_id = f\"S{i}\"\n",
    "            server_id = f\"{station_id}-s0\" # Assumes 1 server 's0'\n",
    "            for k in range(1, 4):\n",
    "                class_id = 3 * (i - 1) + k\n",
    "                queue_id = f\"Q{class_id}\"\n",
    "                actions.append((server_id, queue_id))\n",
    "                \n",
    "        actions.append((\"NULL\", \"NULL\"))\n",
    "        return actions\n",
    "\n",
    "    def _get_action_mask(self, sim_net: Network, free_servers: Dict[str, List[Server]]) -> np.ndarray:\n",
    "        mask = np.zeros(len(self.master_action_list), dtype=int)\n",
    "        \n",
    "        non_empty_queues = {\n",
    "            (sid, qid) for sid, st in sim_net.stations.items()\n",
    "            for qid, q in st.queues.items() if len(q) > 0\n",
    "        }\n",
    "        \n",
    "        found_legal_action = False\n",
    "        for station_id, srvs in free_servers.items():\n",
    "            for srv in srvs:\n",
    "                for action_idx, (srv_id, qid) in enumerate(self.master_action_list):\n",
    "                    if srv.server_id == srv_id:\n",
    "                        if (station_id, qid) in non_empty_queues:\n",
    "                            mask[action_idx] = 1\n",
    "                            found_legal_action = True\n",
    "                            \n",
    "        if not found_legal_action and free_servers:\n",
    "            # If servers are free but no actions are possible,\n",
    "            # the *only* legal action is to do nothing.\n",
    "            mask[self.NULL_ACTION_IDX] = 1\n",
    "            \n",
    "        return mask\n",
    "\n",
    "    def decide(\n",
    "        self,\n",
    "        net: Network,\n",
    "        t: float,\n",
    "        free_servers: Dict[str, List[Server]],\n",
    "    ) -> Tuple[Dict[Server, Queue], MCTSNode, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Runs the full MCTS search and returns the best action,\n",
    "        the root node (for training data), and the state vector.\n",
    "        \"\"\"\n",
    "        \n",
    "        # --- 1. Sequential Assignment ---\n",
    "        # We only assign ONE server at a time.\n",
    "        # We find the *first* free server and make a decision for it.\n",
    "        # The Network will call us again if others are still free.\n",
    "        \n",
    "        server_to_assign: Optional[Server] = None\n",
    "        station_id_for_server: Optional[str] = None \n",
    "        for st_id, srvs in free_servers.items():\n",
    "            if srvs:\n",
    "                server_to_assign = srvs[0]\n",
    "                station_id_for_server = st_id     \n",
    "                break\n",
    "        \n",
    "        if server_to_assign is None:\n",
    "            # No free servers, return empty info\n",
    "            root = MCTSNode()\n",
    "            state_vec = root.get_state_vector(net, self.config[\"MAX_QUEUES_STATE\"])\n",
    "            return {}, root, state_vec\n",
    "        \n",
    "        search_free_servers = {station_id_for_server: [server_to_assign]}\n",
    "\n",
    "        # --- 2. Run MCTS Search ---\n",
    "        try:\n",
    "            real_snapshot = net.clone()\n",
    "        except Exception as e:\n",
    "            print(f\"CRITICAL ERROR: Failed to clone network: {e}\")\n",
    "            root = MCTSNode()\n",
    "            state_vec = root.get_state_vector(net, self.config[\"MAX_QUEUES_STATE\"])\n",
    "            return {}, root, state_vec # Fallback: do nothing\n",
    "\n",
    "        root = MCTSNode()\n",
    "        # Get the state vector for this root *before* search\n",
    "        state_vec = root.get_state_vector(net, self.config[\"MAX_QUEUES_STATE\"])\n",
    "        \n",
    "        for _ in range(self.config[\"mcts_simulations\"]):\n",
    "            node = root\n",
    "            sim_net = real_snapshot.clone()\n",
    "            sim_t = t\n",
    "            sim_free_servers = search_free_servers\n",
    "            \n",
    "            # --- SELECTION ---\n",
    "            path = [node]\n",
    "            while node.children: \n",
    "                action_idx, node = node.select_child_puct(self.config[\"c_puct\"])\n",
    "                path.append(node)\n",
    "                \n",
    "                (sim_net, sim_t, sim_free_servers, is_terminal) = self._run_sim_step(\n",
    "                    sim_net, \n",
    "                    self.idx_to_action[action_idx]\n",
    "                )\n",
    "                \n",
    "                if is_terminal:\n",
    "                    node._is_terminal = True\n",
    "                    break\n",
    "            \n",
    "            # --- EXPANSION & EVALUATION ---\n",
    "            value = 0.0 # Default terminal value\n",
    "            if not node._is_terminal:\n",
    "                leaf_state_vec = node.get_state_vector(sim_net, self.config[\"MAX_QUEUES_STATE\"])\n",
    "                state_tensor = torch.tensor(leaf_state_vec, dtype=torch.float32).unsqueeze(0).to(self.device)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    policy_logits, value_tensor = self.model(state_tensor)\n",
    "                \n",
    "                value = value_tensor.item()\n",
    "                action_mask = self._get_action_mask(sim_net, sim_free_servers)\n",
    "                \n",
    "                policy_logits = policy_logits.squeeze(0)\n",
    "                policy_logits[action_mask == 0] = -torch.inf\n",
    "                policy_probs = F.softmax(policy_logits, dim=0).cpu().numpy()\n",
    "                \n",
    "                node.expand(policy_probs)\n",
    "            \n",
    "            # --- BACKPROPAGATION ---\n",
    "            node.backpropagate(value)\n",
    "\n",
    "        # --- 3. Make Final Decision ---\n",
    "        if not root.children:\n",
    "            best_action_idx = self.NULL_ACTION_IDX\n",
    "        else:\n",
    "            # Select action based on visit counts\n",
    "            best_action_idx = max(\n",
    "                root.children, \n",
    "                key=lambda action_idx: root.children[action_idx].N\n",
    "            )\n",
    "        \n",
    "        best_action: Tuple[str, str] = self.idx_to_action[best_action_idx]\n",
    "        \n",
    "        # --- 4. Translate Action to Assignment ---\n",
    "        assignments: Dict[Server, Queue] = {}\n",
    "        (srv_id, qid) = best_action\n",
    "        \n",
    "        if srv_id != \"NULL\":\n",
    "            # Find the actual Server object\n",
    "            srv = server_to_assign\n",
    "            if srv.server_id == srv_id:\n",
    "                # <-- FIX 4: Use the stored station ID to find the queue\n",
    "                q = net.stations[station_id_for_server].queues[qid] \n",
    "                assignments[srv] = q\n",
    "            else:\n",
    "                # This is a logic error\n",
    "                print(f\"Warning: MCTS chose action for wrong server! {srv_id}\")\n",
    "\n",
    "        return assignments, root, state_vec\n",
    "\n",
    "    def _run_sim_step(\n",
    "        self, \n",
    "        sim_net: Network, \n",
    "        action: Tuple[str, str]\n",
    "    ) -> Tuple[Network, float, Dict[str, List[Server]], bool]:\n",
    "        \"\"\"Applies one MCTS action and runs sim to next decision.\"\"\"\n",
    "        \n",
    "        action_policy = FixedActionPolicy(action)\n",
    "        sim_net.policy = action_policy\n",
    "        \n",
    "        free_now = sim_net._get_free_servers()\n",
    "        if free_now:\n",
    "            assignments = action_policy.decide(sim_net, sim_net.t, free_now)\n",
    "            for srv, q in assignments.items():\n",
    "                if len(q) == 0 or srv.busy: continue\n",
    "                job = q.pop()\n",
    "                dep_time = srv.start_service(job, sim_net.t)\n",
    "                st_id = q.station_id\n",
    "                st = sim_net.stations[st_id]\n",
    "                server_idx = st.servers.index(srv)\n",
    "                sim_net.schedule(dep_time, EventType.DEPARTURE, {\"station_id\": st_id, \"server_idx\": server_idx})\n",
    "        \n",
    "        t_start = sim_net.t\n",
    "        (new_t, new_free_servers) = sim_net.run_until_next_decision()\n",
    "        \n",
    "        is_terminal = False\n",
    "        if not sim_net._event_q: # Check if event queue is empty\n",
    "            is_terminal = True\n",
    "        if new_t > t_start + self.config[\"mcts_sim_horizon_s\"]:\n",
    "            is_terminal = True\n",
    "            \n",
    "        return sim_net, new_t, new_free_servers, is_terminal\n",
    "    \n",
    "    ### --- MODIFIED (Made Public) --- ###\n",
    "    def get_policy_target(self, root: MCTSNode, temperature: float) -> np.ndarray:\n",
    "        \"\"\"Get the policy target (visit counts) to train the NN.\"\"\"\n",
    "        policy_target = np.zeros(len(self.master_action_list))\n",
    "        if not root.children:\n",
    "            policy_target[self.NULL_ACTION_IDX] = 1.0\n",
    "            return policy_target\n",
    "            \n",
    "        visit_counts = np.array([\n",
    "            child.N for child in root.children.values()\n",
    "        ])\n",
    "        action_indices = np.array([\n",
    "            action_idx for action_idx in root.children.keys()\n",
    "        ])\n",
    "        \n",
    "        if temperature == 0:\n",
    "            best_action_local_idx = np.argmax(visit_counts)\n",
    "            best_action_global_idx = action_indices[best_action_local_idx]\n",
    "            policy_target[best_action_global_idx] = 1.0\n",
    "        else:\n",
    "            visit_counts_temp = visit_counts ** (1.0 / temperature)\n",
    "            probs_sum = np.sum(visit_counts_temp)\n",
    "            if probs_sum == 0:\n",
    "                # All counts are 0, use uniform\n",
    "                for idx in action_indices:\n",
    "                    policy_target[idx] = 1.0 / len(action_indices)\n",
    "            else:\n",
    "                probs = visit_counts_temp / probs_sum\n",
    "                for idx, prob in zip(action_indices, probs):\n",
    "                    policy_target[idx] = prob\n",
    "                \n",
    "        return policy_target\n",
    "\n",
    "# --- 5. Helper Policy for MCTS ---\n",
    "\n",
    "class FixedActionPolicy(SchedulingPolicy):\n",
    "    \"\"\"A dummy policy that executes one pre-selected action.\"\"\"\n",
    "    def __init__(self, action: Tuple[str, str]):\n",
    "        self.srv_id, self.qid = action\n",
    "\n",
    "    def decide(\n",
    "        self, \n",
    "        net: Network, \n",
    "        t: float, \n",
    "        free_servers: Dict[str, List[Server]]\n",
    "    ) -> Dict[Server, Queue]:\n",
    "        \n",
    "        assignments = {}\n",
    "        if self.srv_id == \"NULL\":\n",
    "            return assignments\n",
    "            \n",
    "        for st_id, srvs in free_servers.items():\n",
    "            for srv in srvs:\n",
    "                if srv.server_id == self.srv_id:\n",
    "                    q = net.stations[st_id].queues[self.qid]\n",
    "                    assignments[srv] = q\n",
    "                    return assignments\n",
    "        return assignments\n",
    "\n",
    "# --- 6. The New 1-Player Trainer ---\n",
    "\n",
    "class Trainer:\n",
    "    \"\"\"The orchestrator that manages training.\"\"\"\n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        self.config = config\n",
    "        self.device = config[\"device\"]\n",
    "        self.seed = config[\"seed\"]\n",
    "        random.seed(self.seed)\n",
    "        np.random.seed(self.seed)\n",
    "        torch.manual_seed(self.seed)\n",
    "\n",
    "        temp_policy = MCTS_Policy(None, self.config) # Dummy to get sizes\n",
    "        self.action_space_size = len(temp_policy.master_action_list)\n",
    "        self.state_size = self.config[\"MAX_QUEUES_STATE\"]\n",
    "        \n",
    "        print(f\"Trainer init: state_size={self.state_size}, action_space_size={self.action_space_size}\")\n",
    "\n",
    "        self.model = AlphaZeroNN(\n",
    "            self.state_size, \n",
    "            self.action_space_size\n",
    "        ).to(self.device)\n",
    "        \n",
    "        self.optimizer = optim.Adam(\n",
    "            self.model.parameters(), \n",
    "            lr=config[\"learning_rate\"]\n",
    "        )\n",
    "        self.replay_buffer = ReplayBuffer(config[\"buffer_size\"])\n",
    "        \n",
    "        self.mcts_policy = MCTS_Policy(self.model, self.config)\n",
    "        self.mcts_policy.model.eval()\n",
    "\n",
    "    def run_simulation_episode(self) -> float:\n",
    "        \"\"\"\n",
    "        Simulates one full episode by running its *own* event loop\n",
    "        to capture (state, policy) data at each decision.\n",
    "        \"\"\"\n",
    "        \n",
    "        # 1. Create a new \"real\" simulation\n",
    "        net = ExtendedSixClassNetwork(\n",
    "            policy=self.mcts_policy, # The MCTS policy\n",
    "            L=self.config[\"L\"],\n",
    "            seed=random.randint(0, 1_000_000)\n",
    "        )\n",
    "        \n",
    "        episode_history: List[Tuple[np.ndarray, np.ndarray]] = []\n",
    "        \n",
    "        # 2. Seed the simulation (from Network.run())\n",
    "        if not net._seeded:\n",
    "            for ap in net.arrivals:\n",
    "                t_next = ap.schedule_next(net.t)\n",
    "                net.schedule(t_next, EventType.ARRIVAL, {\"ap\": ap})\n",
    "            net._seeded = True\n",
    "        \n",
    "        # 3. Run the \"Outer\" simulation event loop\n",
    "        while net._event_q:\n",
    "            if net._event_q[0].time > self.config[\"sim_run_duration\"]:\n",
    "                break\n",
    "    \n",
    "            ev = heapq.heappop(net._event_q)\n",
    "    \n",
    "            # --- This logic is copied from Network.run() ---\n",
    "            dt = ev.time - net.t\n",
    "            if dt > 0:\n",
    "                for st in net.stations.values():\n",
    "                    for qid, q in st.queues.items():\n",
    "                        st._ql_area[qid] += len(q) * dt\n",
    "                    num_busy_servers = sum(1 for srv in st.servers if srv.busy)\n",
    "                    st._sl_area += num_busy_servers * dt\n",
    "            net.t = ev.time\n",
    "            # --- End copied logic ---\n",
    "    \n",
    "            # Handle event\n",
    "            if ev.type == EventType.ARRIVAL:\n",
    "                net._on_arrival(ev.payload[\"ap\"])\n",
    "            elif ev.type == EventType.DEPARTURE:\n",
    "                net._on_departure(ev.payload[\"station_id\"], ev.payload[\"server_idx\"])\n",
    "    \n",
    "            # --- DATA CAPTURE HOOK ---\n",
    "            # This is the \"Scheduling decision\" part\n",
    "            free_servers = net._get_free_servers()\n",
    "            \n",
    "            while free_servers:\n",
    "                # 1. A decision is needed. Call MCTS.\n",
    "                assignments, root, state_vec = self.mcts_policy.decide(\n",
    "                    net, net.t, free_servers\n",
    "                )\n",
    "                \n",
    "                # 2. Get the training data\n",
    "                policy_target = self.mcts_policy.get_policy_target(\n",
    "                    root, self.config[\"temperature\"]\n",
    "                )\n",
    "                \n",
    "                # 3. Store for later\n",
    "                episode_history.append((state_vec, policy_target))\n",
    "                \n",
    "                if not assignments:\n",
    "                    # MCTS returned \"NULL\" action\n",
    "                    break # Exit the 'while free_servers' loop\n",
    "                \n",
    "                # 4. Apply the *single* assignment\n",
    "                for srv, q in assignments.items():\n",
    "                    if len(q) == 0 or srv.busy: continue\n",
    "                    job = q.pop()\n",
    "                    dep_time = srv.start_service(job, net.t)\n",
    "                    st_id = q.station_id\n",
    "                    st = net.stations[st_id]\n",
    "                    server_idx = st.servers.index(srv)\n",
    "                    net.schedule(dep_time, EventType.DEPARTURE, {\"station_id\": st_id, \"server_idx\": server_idx})\n",
    "\n",
    "                # 5. Check for more free servers *immediately*\n",
    "                free_servers = net._get_free_servers()\n",
    "                # Loop continues until all servers are assigned\n",
    "        \n",
    "        # 4. Episode is over, get final outcome (z)\n",
    "        final_outcome_z, mean_sojourn = self.get_final_outcome(net)\n",
    "        \n",
    "        # 5. Add all steps to replay buffer with the final outcome\n",
    "        if not episode_history:\n",
    "            print(\"Warning: Episode ended with 0 decisions made.\")\n",
    "            return 0.0\n",
    "\n",
    "        for state, policy_target in episode_history:\n",
    "            self.replay_buffer.push(state, policy_target, final_outcome_z)\n",
    "            \n",
    "        return final_outcome_z, mean_sojourn\n",
    "\n",
    "    def get_final_outcome(self, net: Network) -> float:\n",
    "        \"\"\"\n",
    "        Gets the final score (mean sojourn time) and maps it to [-1, 1].\n",
    "        +1 is good (0 sojourn), -1 is bad (catastrophe).\n",
    "        \"\"\"\n",
    "        mean_sojourn = net.mean_sojourn()\n",
    "\n",
    "        # After experimenting, i think sticking to mean_sojourn is better\n",
    "        if np.isnan(mean_sojourn):\n",
    "            return 0.0 # Neutral score if no jobs completed\n",
    "        \n",
    "        catastrophe_val = self.config[\"CATASTROPHE_SOJOURN_TIME\"] #Needs more thinking\n",
    "        \n",
    "        # Normalize:\n",
    "        # 1. Clip sojourn time between 0 and \"catastrophe\"\n",
    "        clipped_sojourn = np.clip(mean_sojourn, 0, catastrophe_val)\n",
    "        \n",
    "        # 2. Scale to [0, 1], where 0 is good, 1 is bad\n",
    "        scaled_val = clipped_sojourn / catastrophe_val\n",
    "        \n",
    "        # 3. Map to [-1, 1], where +1 is good, -1 is bad\n",
    "        final_score = 1.0 - (2.0 * scaled_val)\n",
    "        \n",
    "        return float(final_score), mean_sojourn\n",
    "\n",
    "    def train_step(self) -> Optional[float]:\n",
    "        \"\"\"Samples a batch and performs one backprop step.\"\"\"\n",
    "        if len(self.replay_buffer) < self.config[\"batch_size\"]:\n",
    "            return None \n",
    "\n",
    "        states, policy_targets, value_targets = self.replay_buffer.sample(\n",
    "            self.config[\"batch_size\"]\n",
    "        )\n",
    "        \n",
    "        states = torch.tensor(states, dtype=torch.float32).to(self.device)\n",
    "        policy_targets = torch.tensor(policy_targets, dtype=torch.float32).to(self.device)\n",
    "        value_targets = torch.tensor(value_targets, dtype=torch.float32).to(self.device)\n",
    "        \n",
    "        self.model.train() \n",
    "        policy_logits, values = self.model(states)\n",
    "        \n",
    "        value_loss = F.mse_loss(values, value_targets)\n",
    "        policy_loss = F.cross_entropy(policy_logits, policy_targets)\n",
    "        total_loss = value_loss + policy_loss # weight should not be equal\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return total_loss.item()\n",
    "\n",
    "    def run_training_loop(self):\n",
    "        \"\"\"The main entry point to start the whole process.\"\"\"\n",
    "        for i in range(self.config[\"num_train_loops\"]):\n",
    "            print(f\"\\n--- TRAINING LOOP {i+1}/{self.config['num_train_loops']} ---\")\n",
    "            \n",
    "            # 1. Generate new data\n",
    "            self.mcts_policy.model.eval() # Eval mode for self-play\n",
    "            avg_score, avg_raw = 0,0\n",
    "            total_decisions = 0\n",
    "            for j in range(self.config[\"episodes_per_loop\"]):\n",
    "                buffer_before = len(self.replay_buffer)\n",
    "                score, raw = self.run_simulation_episode()\n",
    "                buffer_after = len(self.replay_buffer)\n",
    "                \n",
    "                decisions = buffer_after - buffer_before\n",
    "                total_decisions += decisions\n",
    "                avg_score += score\n",
    "                avg_raw += raw\n",
    "                print(f\"  Episode {j+1}/{self.config['episodes_per_loop']} complete. Score: {score:.3f}. Decisions: {decisions}\")\n",
    "            \n",
    "            print(f\"Average episode score: {avg_score / self.config['episodes_per_loop']:.4f}\")\n",
    "            print(f\"Total decisions added to buffer: {total_decisions}\")\n",
    "            \n",
    "            # 2. Train the model\n",
    "            avg_loss = 0\n",
    "            train_steps = 0\n",
    "            for _ in range(self.config[\"train_steps_per_loop\"]):\n",
    "                loss = self.train_step()\n",
    "                if loss:\n",
    "                    avg_loss += loss\n",
    "                    train_steps += 1\n",
    "            \n",
    "            if train_steps > 0:\n",
    "                print(f\"Average training loss: {avg_loss / train_steps:.4f}\")\n",
    "                print(f\"Average episode size: {avg_raw / self.config['episodes_per_loop']:.4f}\")\n",
    "            else:\n",
    "                print(f\"Buffer size {len(self.replay_buffer)} < batch size {self.config['batch_size']}. Skipping training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3656054a-5160-483b-9267-a28b1c984caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Trainer...\n",
      "MCTS Policy initialized with 7 master actions.\n",
      "Trainer init: state_size=10, action_space_size=7\n",
      "MCTS Policy initialized with 7 master actions.\n",
      "\n",
      "--- Starting Training Loop ---\n",
      "\n",
      "--- TRAINING LOOP 1/100 ---\n",
      "  Episode 1/50 complete. Score: 0.374. Decisions: 205\n",
      "  Episode 2/50 complete. Score: -0.341. Decisions: 209\n",
      "  Episode 3/50 complete. Score: 0.520. Decisions: 188\n",
      "  Episode 4/50 complete. Score: 0.160. Decisions: 210\n",
      "  Episode 5/50 complete. Score: 0.379. Decisions: 188\n",
      "  Episode 6/50 complete. Score: 0.251. Decisions: 173\n",
      "  Episode 7/50 complete. Score: -1.000. Decisions: 171\n",
      "  Episode 8/50 complete. Score: 0.385. Decisions: 173\n",
      "  Episode 9/50 complete. Score: -0.032. Decisions: 178\n",
      "  Episode 10/50 complete. Score: -0.671. Decisions: 179\n",
      "  Episode 11/50 complete. Score: -0.142. Decisions: 214\n",
      "  Episode 12/50 complete. Score: 0.440. Decisions: 220\n",
      "  Episode 13/50 complete. Score: 0.520. Decisions: 190\n",
      "  Episode 14/50 complete. Score: 0.324. Decisions: 233\n",
      "  Episode 15/50 complete. Score: 0.245. Decisions: 196\n",
      "  Episode 16/50 complete. Score: -0.199. Decisions: 189\n",
      "  Episode 17/50 complete. Score: -0.212. Decisions: 193\n",
      "  Episode 18/50 complete. Score: 0.321. Decisions: 216\n",
      "  Episode 19/50 complete. Score: 0.162. Decisions: 200\n",
      "  Episode 20/50 complete. Score: -1.000. Decisions: 174\n",
      "  Episode 21/50 complete. Score: 0.020. Decisions: 222\n",
      "  Episode 22/50 complete. Score: -0.055. Decisions: 198\n",
      "  Episode 23/50 complete. Score: -0.397. Decisions: 170\n",
      "  Episode 24/50 complete. Score: 0.259. Decisions: 199\n",
      "  Episode 25/50 complete. Score: 0.256. Decisions: 203\n",
      "  Episode 26/50 complete. Score: 0.543. Decisions: 212\n",
      "  Episode 27/50 complete. Score: 0.588. Decisions: 217\n",
      "  Episode 28/50 complete. Score: -0.117. Decisions: 197\n",
      "  Episode 29/50 complete. Score: 0.135. Decisions: 196\n",
      "  Episode 30/50 complete. Score: -0.164. Decisions: 212\n",
      "  Episode 31/50 complete. Score: 0.668. Decisions: 235\n",
      "  Episode 32/50 complete. Score: 0.496. Decisions: 233\n",
      "  Episode 33/50 complete. Score: -0.423. Decisions: 175\n",
      "  Episode 34/50 complete. Score: -0.063. Decisions: 229\n",
      "  Episode 35/50 complete. Score: 0.249. Decisions: 205\n",
      "  Episode 36/50 complete. Score: 0.413. Decisions: 194\n",
      "  Episode 37/50 complete. Score: -0.175. Decisions: 196\n",
      "  Episode 38/50 complete. Score: 0.575. Decisions: 231\n",
      "  Episode 39/50 complete. Score: 0.504. Decisions: 210\n",
      "  Episode 40/50 complete. Score: 0.708. Decisions: 219\n",
      "  Episode 41/50 complete. Score: 0.387. Decisions: 223\n",
      "  Episode 42/50 complete. Score: 0.293. Decisions: 216\n",
      "  Episode 43/50 complete. Score: 0.306. Decisions: 203\n",
      "  Episode 44/50 complete. Score: -0.031. Decisions: 201\n",
      "  Episode 45/50 complete. Score: 0.253. Decisions: 188\n",
      "  Episode 46/50 complete. Score: 0.413. Decisions: 199\n",
      "  Episode 47/50 complete. Score: 0.160. Decisions: 197\n",
      "  Episode 48/50 complete. Score: 0.367. Decisions: 197\n",
      "  Episode 49/50 complete. Score: 0.272. Decisions: 215\n",
      "  Episode 50/50 complete. Score: 0.026. Decisions: 188\n",
      "Average episode score: 0.1390\n",
      "Total decisions added to buffer: 10079\n",
      "Average training loss: 1.7066\n",
      "Average episode size: 86.6124\n",
      "\n",
      "--- TRAINING LOOP 2/100 ---\n",
      "  Episode 1/50 complete. Score: 0.596. Decisions: 231\n",
      "  Episode 2/50 complete. Score: 0.325. Decisions: 198\n",
      "  Episode 3/50 complete. Score: 0.110. Decisions: 239\n",
      "  Episode 4/50 complete. Score: 0.315. Decisions: 236\n",
      "  Episode 5/50 complete. Score: 0.156. Decisions: 210\n",
      "  Episode 6/50 complete. Score: 0.154. Decisions: 223\n",
      "  Episode 7/50 complete. Score: 0.470. Decisions: 216\n",
      "  Episode 8/50 complete. Score: 0.343. Decisions: 190\n",
      "  Episode 9/50 complete. Score: 0.201. Decisions: 192\n",
      "  Episode 10/50 complete. Score: 0.523. Decisions: 204\n",
      "  Episode 11/50 complete. Score: 0.421. Decisions: 213\n",
      "  Episode 12/50 complete. Score: 0.292. Decisions: 214\n",
      "  Episode 13/50 complete. Score: 0.326. Decisions: 215\n",
      "  Episode 14/50 complete. Score: 0.049. Decisions: 212\n",
      "  Episode 15/50 complete. Score: 0.224. Decisions: 202\n",
      "  Episode 16/50 complete. Score: 0.094. Decisions: 196\n",
      "  Episode 17/50 complete. Score: 0.358. Decisions: 177\n",
      "  Episode 18/50 complete. Score: 0.449. Decisions: 194\n",
      "  Episode 19/50 complete. Score: 0.367. Decisions: 212\n",
      "  Episode 20/50 complete. Score: 0.364. Decisions: 248\n",
      "  Episode 21/50 complete. Score: 0.066. Decisions: 227\n",
      "  Episode 22/50 complete. Score: 0.226. Decisions: 207\n",
      "  Episode 23/50 complete. Score: 0.299. Decisions: 224\n",
      "  Episode 24/50 complete. Score: 0.399. Decisions: 213\n",
      "  Episode 25/50 complete. Score: -0.139. Decisions: 192\n",
      "  Episode 26/50 complete. Score: 0.410. Decisions: 237\n",
      "  Episode 27/50 complete. Score: -0.323. Decisions: 182\n",
      "  Episode 28/50 complete. Score: 0.211. Decisions: 216\n",
      "  Episode 29/50 complete. Score: 0.280. Decisions: 218\n",
      "  Episode 30/50 complete. Score: 0.569. Decisions: 216\n",
      "  Episode 31/50 complete. Score: -0.347. Decisions: 186\n",
      "  Episode 32/50 complete. Score: 0.496. Decisions: 242\n",
      "  Episode 33/50 complete. Score: 0.319. Decisions: 214\n",
      "  Episode 34/50 complete. Score: 0.345. Decisions: 214\n",
      "  Episode 35/50 complete. Score: 0.226. Decisions: 214\n",
      "  Episode 36/50 complete. Score: 0.128. Decisions: 197\n",
      "  Episode 37/50 complete. Score: 0.430. Decisions: 262\n",
      "  Episode 38/50 complete. Score: 0.226. Decisions: 230\n",
      "  Episode 39/50 complete. Score: 0.422. Decisions: 226\n",
      "  Episode 40/50 complete. Score: 0.069. Decisions: 223\n",
      "  Episode 41/50 complete. Score: -0.275. Decisions: 203\n",
      "  Episode 42/50 complete. Score: 0.335. Decisions: 187\n",
      "  Episode 43/50 complete. Score: 0.661. Decisions: 189\n",
      "  Episode 44/50 complete. Score: 0.302. Decisions: 236\n",
      "  Episode 45/50 complete. Score: 0.536. Decisions: 236\n",
      "  Episode 46/50 complete. Score: 0.035. Decisions: 201\n",
      "  Episode 47/50 complete. Score: 0.159. Decisions: 195\n",
      "  Episode 48/50 complete. Score: 0.313. Decisions: 223\n",
      "  Episode 49/50 complete. Score: 0.002. Decisions: 175\n",
      "  Episode 50/50 complete. Score: 0.521. Decisions: 235\n",
      "Average episode score: 0.2608\n",
      "Total decisions added to buffer: 10642\n",
      "Average training loss: 1.2885\n",
      "Average episode size: 73.9216\n",
      "\n",
      "--- TRAINING LOOP 3/100 ---\n",
      "  Episode 1/50 complete. Score: 0.416. Decisions: 188\n",
      "  Episode 2/50 complete. Score: 0.548. Decisions: 225\n",
      "  Episode 3/50 complete. Score: 0.442. Decisions: 185\n",
      "  Episode 4/50 complete. Score: 0.391. Decisions: 195\n",
      "  Episode 5/50 complete. Score: 0.600. Decisions: 178\n",
      "  Episode 6/50 complete. Score: 0.450. Decisions: 225\n",
      "  Episode 7/50 complete. Score: 0.635. Decisions: 209\n",
      "  Episode 8/50 complete. Score: -0.057. Decisions: 194\n",
      "  Episode 9/50 complete. Score: 0.286. Decisions: 205\n",
      "  Episode 10/50 complete. Score: 0.488. Decisions: 200\n",
      "  Episode 11/50 complete. Score: 0.645. Decisions: 198\n",
      "  Episode 12/50 complete. Score: 0.651. Decisions: 206\n",
      "  Episode 13/50 complete. Score: 0.300. Decisions: 205\n",
      "  Episode 14/50 complete. Score: 0.622. Decisions: 215\n",
      "  Episode 15/50 complete. Score: 0.469. Decisions: 211\n",
      "  Episode 16/50 complete. Score: 0.366. Decisions: 209\n",
      "  Episode 17/50 complete. Score: -0.053. Decisions: 189\n",
      "  Episode 18/50 complete. Score: 0.611. Decisions: 229\n",
      "  Episode 19/50 complete. Score: 0.134. Decisions: 221\n",
      "  Episode 20/50 complete. Score: 0.377. Decisions: 213\n",
      "  Episode 21/50 complete. Score: 0.425. Decisions: 216\n",
      "  Episode 22/50 complete. Score: 0.431. Decisions: 220\n",
      "  Episode 23/50 complete. Score: 0.369. Decisions: 220\n",
      "  Episode 24/50 complete. Score: 0.665. Decisions: 206\n",
      "  Episode 25/50 complete. Score: 0.415. Decisions: 241\n",
      "  Episode 26/50 complete. Score: 0.462. Decisions: 188\n",
      "  Episode 27/50 complete. Score: 0.225. Decisions: 190\n",
      "  Episode 28/50 complete. Score: 0.693. Decisions: 223\n",
      "  Episode 29/50 complete. Score: 0.500. Decisions: 220\n",
      "  Episode 30/50 complete. Score: 0.573. Decisions: 239\n",
      "  Episode 31/50 complete. Score: 0.028. Decisions: 206\n",
      "  Episode 32/50 complete. Score: 0.423. Decisions: 245\n",
      "  Episode 33/50 complete. Score: 0.565. Decisions: 182\n",
      "  Episode 34/50 complete. Score: 0.575. Decisions: 223\n",
      "  Episode 35/50 complete. Score: 0.554. Decisions: 221\n",
      "  Episode 36/50 complete. Score: 0.721. Decisions: 214\n",
      "  Episode 37/50 complete. Score: 0.608. Decisions: 211\n",
      "  Episode 38/50 complete. Score: 0.509. Decisions: 209\n",
      "  Episode 39/50 complete. Score: 0.274. Decisions: 205\n",
      "  Episode 40/50 complete. Score: 0.441. Decisions: 237\n",
      "  Episode 41/50 complete. Score: 0.314. Decisions: 217\n",
      "  Episode 42/50 complete. Score: 0.241. Decisions: 210\n",
      "  Episode 43/50 complete. Score: 0.264. Decisions: 204\n",
      "  Episode 44/50 complete. Score: 0.574. Decisions: 207\n",
      "  Episode 45/50 complete. Score: 0.595. Decisions: 202\n",
      "  Episode 46/50 complete. Score: 0.234. Decisions: 215\n",
      "  Episode 47/50 complete. Score: 0.092. Decisions: 213\n",
      "  Episode 48/50 complete. Score: 0.426. Decisions: 199\n",
      "  Episode 49/50 complete. Score: 0.538. Decisions: 234\n",
      "  Episode 50/50 complete. Score: 0.109. Decisions: 206\n",
      "Average episode score: 0.4233\n",
      "Total decisions added to buffer: 10523\n",
      "Average training loss: 1.1647\n",
      "Average episode size: 57.6713\n",
      "\n",
      "--- TRAINING LOOP 4/100 ---\n",
      "  Episode 1/50 complete. Score: 0.381. Decisions: 226\n",
      "  Episode 2/50 complete. Score: 0.347. Decisions: 187\n",
      "  Episode 3/50 complete. Score: 0.598. Decisions: 211\n",
      "  Episode 4/50 complete. Score: 0.555. Decisions: 205\n",
      "  Episode 5/50 complete. Score: 0.583. Decisions: 220\n",
      "  Episode 6/50 complete. Score: 0.231. Decisions: 186\n",
      "  Episode 7/50 complete. Score: 0.246. Decisions: 205\n",
      "  Episode 8/50 complete. Score: 0.142. Decisions: 196\n",
      "  Episode 9/50 complete. Score: 0.355. Decisions: 220\n",
      "  Episode 10/50 complete. Score: 0.417. Decisions: 221\n",
      "  Episode 11/50 complete. Score: 0.377. Decisions: 210\n",
      "  Episode 12/50 complete. Score: 0.170. Decisions: 225\n",
      "  Episode 13/50 complete. Score: 0.309. Decisions: 225\n",
      "  Episode 14/50 complete. Score: -0.020. Decisions: 210\n",
      "  Episode 15/50 complete. Score: 0.241. Decisions: 188\n",
      "  Episode 16/50 complete. Score: 0.375. Decisions: 206\n",
      "  Episode 17/50 complete. Score: 0.328. Decisions: 228\n",
      "  Episode 18/50 complete. Score: 0.163. Decisions: 178\n",
      "  Episode 19/50 complete. Score: 0.440. Decisions: 240\n",
      "  Episode 20/50 complete. Score: 0.532. Decisions: 225\n",
      "  Episode 21/50 complete. Score: 0.327. Decisions: 237\n",
      "  Episode 22/50 complete. Score: 0.462. Decisions: 209\n",
      "  Episode 23/50 complete. Score: 0.550. Decisions: 196\n",
      "  Episode 24/50 complete. Score: 0.401. Decisions: 224\n",
      "  Episode 25/50 complete. Score: 0.213. Decisions: 222\n",
      "  Episode 26/50 complete. Score: 0.291. Decisions: 216\n",
      "  Episode 27/50 complete. Score: 0.239. Decisions: 199\n",
      "  Episode 28/50 complete. Score: 0.494. Decisions: 198\n",
      "  Episode 29/50 complete. Score: 0.508. Decisions: 217\n",
      "  Episode 30/50 complete. Score: 0.639. Decisions: 197\n",
      "  Episode 31/50 complete. Score: 0.115. Decisions: 235\n",
      "  Episode 32/50 complete. Score: 0.150. Decisions: 229\n",
      "  Episode 33/50 complete. Score: 0.676. Decisions: 186\n",
      "  Episode 34/50 complete. Score: 0.103. Decisions: 211\n",
      "  Episode 35/50 complete. Score: 0.344. Decisions: 221\n",
      "  Episode 36/50 complete. Score: 0.286. Decisions: 189\n",
      "  Episode 37/50 complete. Score: 0.517. Decisions: 184\n",
      "  Episode 38/50 complete. Score: 0.588. Decisions: 248\n",
      "  Episode 39/50 complete. Score: 0.557. Decisions: 237\n",
      "  Episode 40/50 complete. Score: 0.419. Decisions: 229\n",
      "  Episode 41/50 complete. Score: 0.553. Decisions: 243\n",
      "  Episode 42/50 complete. Score: 0.101. Decisions: 173\n",
      "  Episode 43/50 complete. Score: 0.492. Decisions: 229\n",
      "  Episode 44/50 complete. Score: 0.383. Decisions: 205\n",
      "  Episode 45/50 complete. Score: 0.422. Decisions: 233\n",
      "  Episode 46/50 complete. Score: 0.410. Decisions: 253\n",
      "  Episode 47/50 complete. Score: 0.539. Decisions: 198\n",
      "  Episode 48/50 complete. Score: 0.386. Decisions: 190\n",
      "  Episode 49/50 complete. Score: 0.264. Decisions: 196\n",
      "  Episode 50/50 complete. Score: 0.457. Decisions: 205\n",
      "Average episode score: 0.3731\n",
      "Total decisions added to buffer: 10621\n",
      "Average training loss: 1.1339\n",
      "Average episode size: 62.6874\n",
      "\n",
      "--- TRAINING LOOP 5/100 ---\n",
      "  Episode 1/50 complete. Score: 0.352. Decisions: 208\n",
      "  Episode 2/50 complete. Score: 0.622. Decisions: 206\n",
      "  Episode 3/50 complete. Score: 0.600. Decisions: 199\n",
      "  Episode 4/50 complete. Score: 0.055. Decisions: 231\n",
      "  Episode 5/50 complete. Score: 0.406. Decisions: 206\n",
      "  Episode 6/50 complete. Score: 0.470. Decisions: 230\n",
      "  Episode 7/50 complete. Score: 0.360. Decisions: 210\n",
      "  Episode 8/50 complete. Score: 0.390. Decisions: 223\n",
      "  Episode 9/50 complete. Score: 0.550. Decisions: 253\n",
      "  Episode 10/50 complete. Score: 0.516. Decisions: 217\n",
      "  Episode 11/50 complete. Score: 0.326. Decisions: 219\n",
      "  Episode 12/50 complete. Score: 0.502. Decisions: 228\n",
      "  Episode 13/50 complete. Score: 0.106. Decisions: 183\n",
      "  Episode 14/50 complete. Score: 0.310. Decisions: 207\n",
      "  Episode 15/50 complete. Score: 0.501. Decisions: 221\n",
      "  Episode 16/50 complete. Score: 0.118. Decisions: 182\n",
      "  Episode 17/50 complete. Score: 0.192. Decisions: 217\n",
      "  Episode 18/50 complete. Score: 0.175. Decisions: 212\n",
      "  Episode 19/50 complete. Score: 0.482. Decisions: 214\n",
      "  Episode 20/50 complete. Score: 0.133. Decisions: 228\n",
      "  Episode 21/50 complete. Score: 0.463. Decisions: 213\n",
      "  Episode 22/50 complete. Score: 0.209. Decisions: 185\n",
      "  Episode 23/50 complete. Score: 0.659. Decisions: 218\n",
      "  Episode 24/50 complete. Score: 0.526. Decisions: 237\n",
      "  Episode 25/50 complete. Score: 0.568. Decisions: 217\n",
      "  Episode 26/50 complete. Score: 0.425. Decisions: 196\n",
      "  Episode 27/50 complete. Score: 0.404. Decisions: 231\n",
      "  Episode 28/50 complete. Score: 0.412. Decisions: 190\n",
      "  Episode 29/50 complete. Score: 0.271. Decisions: 211\n",
      "  Episode 30/50 complete. Score: 0.288. Decisions: 203\n",
      "  Episode 31/50 complete. Score: 0.395. Decisions: 216\n",
      "  Episode 32/50 complete. Score: 0.421. Decisions: 223\n",
      "  Episode 33/50 complete. Score: 0.150. Decisions: 168\n",
      "  Episode 34/50 complete. Score: 0.612. Decisions: 226\n",
      "  Episode 35/50 complete. Score: 0.101. Decisions: 216\n",
      "  Episode 36/50 complete. Score: 0.595. Decisions: 243\n",
      "  Episode 37/50 complete. Score: 0.436. Decisions: 211\n",
      "  Episode 38/50 complete. Score: -0.065. Decisions: 182\n",
      "  Episode 39/50 complete. Score: 0.461. Decisions: 55\n",
      "  Episode 40/50 complete. Score: 0.252. Decisions: 0\n",
      "  Episode 41/50 complete. Score: 0.638. Decisions: 0\n",
      "  Episode 42/50 complete. Score: 0.608. Decisions: 0\n",
      "  Episode 43/50 complete. Score: 0.566. Decisions: 0\n",
      "  Episode 44/50 complete. Score: 0.078. Decisions: 0\n",
      "  Episode 45/50 complete. Score: 0.064. Decisions: 0\n",
      "  Episode 46/50 complete. Score: 0.379. Decisions: 0\n",
      "  Episode 47/50 complete. Score: 0.518. Decisions: 0\n",
      "  Episode 48/50 complete. Score: 0.403. Decisions: 0\n",
      "  Episode 49/50 complete. Score: 0.516. Decisions: 0\n",
      "  Episode 50/50 complete. Score: 0.622. Decisions: 0\n",
      "Average episode score: 0.3828\n",
      "Total decisions added to buffer: 8135\n",
      "Average training loss: 1.0874\n",
      "Average episode size: 61.7223\n",
      "\n",
      "--- TRAINING LOOP 6/100 ---\n",
      "  Episode 1/50 complete. Score: 0.467. Decisions: 0\n",
      "  Episode 2/50 complete. Score: 0.328. Decisions: 0\n",
      "  Episode 3/50 complete. Score: 0.611. Decisions: 0\n",
      "  Episode 4/50 complete. Score: 0.320. Decisions: 0\n",
      "  Episode 5/50 complete. Score: -0.293. Decisions: 0\n",
      "  Episode 6/50 complete. Score: 0.025. Decisions: 0\n",
      "  Episode 7/50 complete. Score: 0.517. Decisions: 0\n",
      "  Episode 8/50 complete. Score: 0.443. Decisions: 0\n",
      "  Episode 9/50 complete. Score: 0.515. Decisions: 0\n",
      "  Episode 10/50 complete. Score: 0.500. Decisions: 0\n",
      "  Episode 11/50 complete. Score: 0.521. Decisions: 0\n",
      "  Episode 12/50 complete. Score: 0.332. Decisions: 0\n",
      "  Episode 13/50 complete. Score: 0.638. Decisions: 0\n",
      "  Episode 14/50 complete. Score: 0.474. Decisions: 0\n",
      "  Episode 15/50 complete. Score: 0.296. Decisions: 0\n",
      "  Episode 16/50 complete. Score: -0.013. Decisions: 0\n",
      "  Episode 17/50 complete. Score: 0.614. Decisions: 0\n",
      "  Episode 18/50 complete. Score: 0.351. Decisions: 0\n",
      "  Episode 19/50 complete. Score: -0.077. Decisions: 0\n",
      "  Episode 20/50 complete. Score: 0.302. Decisions: 0\n",
      "  Episode 21/50 complete. Score: 0.060. Decisions: 0\n",
      "  Episode 22/50 complete. Score: 0.390. Decisions: 0\n",
      "  Episode 23/50 complete. Score: 0.454. Decisions: 0\n",
      "  Episode 24/50 complete. Score: 0.086. Decisions: 0\n",
      "  Episode 25/50 complete. Score: 0.356. Decisions: 0\n",
      "  Episode 26/50 complete. Score: 0.317. Decisions: 0\n",
      "  Episode 27/50 complete. Score: 0.750. Decisions: 0\n",
      "  Episode 28/50 complete. Score: -0.246. Decisions: 0\n",
      "  Episode 29/50 complete. Score: 0.347. Decisions: 0\n",
      "  Episode 30/50 complete. Score: 0.309. Decisions: 0\n",
      "  Episode 31/50 complete. Score: 0.322. Decisions: 0\n",
      "  Episode 32/50 complete. Score: 0.727. Decisions: 0\n",
      "  Episode 33/50 complete. Score: 0.380. Decisions: 0\n",
      "  Episode 34/50 complete. Score: 0.394. Decisions: 0\n",
      "  Episode 35/50 complete. Score: 0.343. Decisions: 0\n",
      "  Episode 36/50 complete. Score: 0.341. Decisions: 0\n",
      "  Episode 37/50 complete. Score: 0.219. Decisions: 0\n",
      "  Episode 38/50 complete. Score: 0.406. Decisions: 0\n",
      "  Episode 39/50 complete. Score: 0.373. Decisions: 0\n",
      "  Episode 40/50 complete. Score: 0.608. Decisions: 0\n",
      "  Episode 41/50 complete. Score: 0.470. Decisions: 0\n",
      "  Episode 42/50 complete. Score: 0.603. Decisions: 0\n",
      "  Episode 43/50 complete. Score: 0.639. Decisions: 0\n",
      "  Episode 44/50 complete. Score: 0.471. Decisions: 0\n",
      "  Episode 45/50 complete. Score: 0.140. Decisions: 0\n",
      "  Episode 46/50 complete. Score: 0.603. Decisions: 0\n",
      "  Episode 47/50 complete. Score: 0.296. Decisions: 0\n",
      "  Episode 48/50 complete. Score: 0.352. Decisions: 0\n",
      "  Episode 49/50 complete. Score: -0.068. Decisions: 0\n",
      "  Episode 50/50 complete. Score: 0.712. Decisions: 0\n",
      "Average episode score: 0.3604\n",
      "Total decisions added to buffer: 0\n",
      "Average training loss: 1.0668\n",
      "Average episode size: 63.9611\n",
      "\n",
      "--- TRAINING LOOP 7/100 ---\n",
      "  Episode 1/50 complete. Score: 0.554. Decisions: 0\n",
      "  Episode 2/50 complete. Score: 0.422. Decisions: 0\n",
      "  Episode 3/50 complete. Score: 0.442. Decisions: 0\n",
      "  Episode 4/50 complete. Score: 0.178. Decisions: 0\n",
      "  Episode 5/50 complete. Score: 0.329. Decisions: 0\n",
      "  Episode 6/50 complete. Score: 0.531. Decisions: 0\n",
      "  Episode 7/50 complete. Score: 0.353. Decisions: 0\n",
      "  Episode 8/50 complete. Score: -0.105. Decisions: 0\n",
      "  Episode 9/50 complete. Score: 0.637. Decisions: 0\n",
      "  Episode 10/50 complete. Score: 0.575. Decisions: 0\n",
      "  Episode 11/50 complete. Score: 0.143. Decisions: 0\n",
      "  Episode 12/50 complete. Score: 0.712. Decisions: 0\n",
      "  Episode 13/50 complete. Score: 0.256. Decisions: 0\n",
      "  Episode 14/50 complete. Score: 0.232. Decisions: 0\n",
      "  Episode 15/50 complete. Score: -0.664. Decisions: 0\n",
      "  Episode 16/50 complete. Score: 0.059. Decisions: 0\n",
      "  Episode 17/50 complete. Score: 0.468. Decisions: 0\n",
      "  Episode 18/50 complete. Score: 0.566. Decisions: 0\n",
      "  Episode 19/50 complete. Score: 0.395. Decisions: 0\n",
      "  Episode 20/50 complete. Score: 0.530. Decisions: 0\n",
      "  Episode 21/50 complete. Score: 0.517. Decisions: 0\n",
      "  Episode 22/50 complete. Score: 0.297. Decisions: 0\n",
      "  Episode 23/50 complete. Score: 0.599. Decisions: 0\n",
      "  Episode 24/50 complete. Score: 0.097. Decisions: 0\n",
      "  Episode 25/50 complete. Score: 0.489. Decisions: 0\n",
      "  Episode 26/50 complete. Score: 0.274. Decisions: 0\n",
      "  Episode 27/50 complete. Score: 0.508. Decisions: 0\n",
      "  Episode 28/50 complete. Score: 0.243. Decisions: 0\n",
      "  Episode 29/50 complete. Score: 0.433. Decisions: 0\n",
      "  Episode 30/50 complete. Score: 0.250. Decisions: 0\n",
      "  Episode 31/50 complete. Score: 0.707. Decisions: 0\n",
      "  Episode 32/50 complete. Score: 0.401. Decisions: 0\n",
      "  Episode 33/50 complete. Score: 0.179. Decisions: 0\n",
      "  Episode 34/50 complete. Score: 0.663. Decisions: 0\n",
      "  Episode 35/50 complete. Score: 0.668. Decisions: 0\n",
      "  Episode 36/50 complete. Score: 0.621. Decisions: 0\n",
      "  Episode 37/50 complete. Score: 0.392. Decisions: 0\n",
      "  Episode 38/50 complete. Score: 0.341. Decisions: 0\n",
      "  Episode 39/50 complete. Score: 0.612. Decisions: 0\n",
      "  Episode 40/50 complete. Score: 0.588. Decisions: 0\n",
      "  Episode 41/50 complete. Score: 0.484. Decisions: 0\n",
      "  Episode 42/50 complete. Score: 0.321. Decisions: 0\n",
      "  Episode 43/50 complete. Score: 0.486. Decisions: 0\n",
      "  Episode 44/50 complete. Score: 0.228. Decisions: 0\n",
      "  Episode 45/50 complete. Score: 0.234. Decisions: 0\n",
      "  Episode 46/50 complete. Score: 0.413. Decisions: 0\n",
      "  Episode 47/50 complete. Score: 0.453. Decisions: 0\n",
      "  Episode 48/50 complete. Score: 0.193. Decisions: 0\n",
      "  Episode 49/50 complete. Score: 0.492. Decisions: 0\n",
      "  Episode 50/50 complete. Score: 0.485. Decisions: 0\n",
      "Average episode score: 0.3856\n",
      "Total decisions added to buffer: 0\n",
      "Average training loss: 1.0482\n",
      "Average episode size: 61.4403\n",
      "\n",
      "--- TRAINING LOOP 8/100 ---\n",
      "  Episode 1/50 complete. Score: 0.238. Decisions: 0\n",
      "  Episode 2/50 complete. Score: 0.416. Decisions: 0\n",
      "  Episode 3/50 complete. Score: 0.367. Decisions: 0\n",
      "  Episode 4/50 complete. Score: 0.375. Decisions: 0\n",
      "  Episode 5/50 complete. Score: 0.424. Decisions: 0\n",
      "  Episode 6/50 complete. Score: 0.182. Decisions: 0\n",
      "  Episode 7/50 complete. Score: 0.638. Decisions: 0\n",
      "  Episode 8/50 complete. Score: -0.020. Decisions: 0\n",
      "  Episode 9/50 complete. Score: 0.230. Decisions: 0\n",
      "  Episode 10/50 complete. Score: 0.089. Decisions: 0\n",
      "  Episode 11/50 complete. Score: 0.119. Decisions: 0\n",
      "  Episode 12/50 complete. Score: 0.305. Decisions: 0\n",
      "  Episode 13/50 complete. Score: 0.632. Decisions: 0\n",
      "  Episode 14/50 complete. Score: 0.587. Decisions: 0\n",
      "  Episode 15/50 complete. Score: 0.323. Decisions: 0\n",
      "  Episode 16/50 complete. Score: 0.485. Decisions: 0\n",
      "  Episode 17/50 complete. Score: 0.255. Decisions: 0\n",
      "  Episode 18/50 complete. Score: 0.520. Decisions: 0\n",
      "  Episode 19/50 complete. Score: 0.353. Decisions: 0\n",
      "  Episode 20/50 complete. Score: 0.374. Decisions: 0\n",
      "  Episode 21/50 complete. Score: 0.658. Decisions: 0\n",
      "  Episode 22/50 complete. Score: 0.261. Decisions: 0\n",
      "  Episode 23/50 complete. Score: 0.176. Decisions: 0\n",
      "  Episode 24/50 complete. Score: 0.282. Decisions: 0\n",
      "  Episode 25/50 complete. Score: 0.468. Decisions: 0\n",
      "  Episode 26/50 complete. Score: -0.035. Decisions: 0\n",
      "  Episode 27/50 complete. Score: 0.524. Decisions: 0\n",
      "  Episode 28/50 complete. Score: 0.417. Decisions: 0\n",
      "  Episode 29/50 complete. Score: 0.211. Decisions: 0\n",
      "  Episode 30/50 complete. Score: -0.278. Decisions: 0\n",
      "  Episode 31/50 complete. Score: 0.110. Decisions: 0\n",
      "  Episode 32/50 complete. Score: 0.437. Decisions: 0\n",
      "  Episode 33/50 complete. Score: 0.564. Decisions: 0\n",
      "  Episode 34/50 complete. Score: 0.604. Decisions: 0\n",
      "  Episode 35/50 complete. Score: 0.032. Decisions: 0\n",
      "  Episode 36/50 complete. Score: 0.401. Decisions: 0\n",
      "  Episode 37/50 complete. Score: 0.631. Decisions: 0\n",
      "  Episode 38/50 complete. Score: 0.162. Decisions: 0\n",
      "  Episode 39/50 complete. Score: 0.492. Decisions: 0\n",
      "  Episode 40/50 complete. Score: 0.443. Decisions: 0\n",
      "  Episode 41/50 complete. Score: 0.317. Decisions: 0\n",
      "  Episode 42/50 complete. Score: 0.520. Decisions: 0\n",
      "  Episode 43/50 complete. Score: 0.011. Decisions: 0\n",
      "  Episode 44/50 complete. Score: 0.336. Decisions: 0\n",
      "  Episode 45/50 complete. Score: -0.096. Decisions: 0\n",
      "  Episode 46/50 complete. Score: 0.553. Decisions: 0\n",
      "  Episode 47/50 complete. Score: 0.218. Decisions: 0\n",
      "  Episode 48/50 complete. Score: 0.536. Decisions: 0\n",
      "  Episode 49/50 complete. Score: 0.236. Decisions: 0\n",
      "  Episode 50/50 complete. Score: 0.303. Decisions: 0\n",
      "Average episode score: 0.3276\n",
      "Total decisions added to buffer: 0\n",
      "Average training loss: 1.0507\n",
      "Average episode size: 67.2382\n",
      "\n",
      "--- TRAINING LOOP 9/100 ---\n",
      "  Episode 1/50 complete. Score: 0.448. Decisions: 0\n",
      "  Episode 2/50 complete. Score: 0.465. Decisions: 0\n",
      "  Episode 3/50 complete. Score: 0.206. Decisions: 0\n",
      "  Episode 4/50 complete. Score: 0.259. Decisions: 0\n",
      "  Episode 5/50 complete. Score: 0.430. Decisions: 0\n",
      "  Episode 6/50 complete. Score: 0.476. Decisions: 0\n",
      "  Episode 7/50 complete. Score: -0.034. Decisions: 0\n",
      "  Episode 8/50 complete. Score: 0.467. Decisions: 0\n",
      "  Episode 9/50 complete. Score: 0.296. Decisions: 0\n",
      "  Episode 10/50 complete. Score: 0.356. Decisions: 0\n",
      "  Episode 11/50 complete. Score: 0.484. Decisions: 0\n",
      "  Episode 12/50 complete. Score: 0.013. Decisions: 0\n",
      "  Episode 13/50 complete. Score: 0.192. Decisions: 0\n",
      "  Episode 14/50 complete. Score: 0.581. Decisions: 0\n",
      "  Episode 15/50 complete. Score: 0.458. Decisions: 0\n",
      "  Episode 16/50 complete. Score: 0.363. Decisions: 0\n",
      "  Episode 17/50 complete. Score: 0.203. Decisions: 0\n",
      "  Episode 18/50 complete. Score: 0.124. Decisions: 0\n",
      "  Episode 19/50 complete. Score: 0.294. Decisions: 0\n",
      "  Episode 20/50 complete. Score: 0.406. Decisions: 0\n",
      "  Episode 21/50 complete. Score: 0.030. Decisions: 0\n",
      "  Episode 22/50 complete. Score: 0.120. Decisions: 0\n",
      "  Episode 23/50 complete. Score: 0.304. Decisions: 0\n",
      "  Episode 24/50 complete. Score: 0.562. Decisions: 0\n",
      "  Episode 25/50 complete. Score: 0.380. Decisions: 0\n",
      "  Episode 26/50 complete. Score: 0.340. Decisions: 0\n",
      "  Episode 27/50 complete. Score: -0.145. Decisions: 0\n",
      "  Episode 28/50 complete. Score: 0.627. Decisions: 0\n",
      "  Episode 29/50 complete. Score: 0.438. Decisions: 0\n",
      "  Episode 30/50 complete. Score: 0.709. Decisions: 0\n",
      "  Episode 31/50 complete. Score: 0.287. Decisions: 0\n",
      "  Episode 32/50 complete. Score: 0.474. Decisions: 0\n",
      "  Episode 33/50 complete. Score: 0.472. Decisions: 0\n",
      "  Episode 34/50 complete. Score: 0.133. Decisions: 0\n",
      "  Episode 35/50 complete. Score: -0.166. Decisions: 0\n",
      "  Episode 36/50 complete. Score: 0.303. Decisions: 0\n",
      "  Episode 37/50 complete. Score: 0.149. Decisions: 0\n",
      "  Episode 38/50 complete. Score: 0.635. Decisions: 0\n",
      "  Episode 39/50 complete. Score: 0.464. Decisions: 0\n",
      "  Episode 40/50 complete. Score: 0.391. Decisions: 0\n",
      "  Episode 41/50 complete. Score: 0.725. Decisions: 0\n",
      "  Episode 42/50 complete. Score: -0.115. Decisions: 0\n",
      "  Episode 43/50 complete. Score: 0.566. Decisions: 0\n",
      "  Episode 44/50 complete. Score: -0.099. Decisions: 0\n",
      "  Episode 45/50 complete. Score: 0.304. Decisions: 0\n",
      "  Episode 46/50 complete. Score: 0.517. Decisions: 0\n",
      "  Episode 47/50 complete. Score: 0.581. Decisions: 0\n",
      "  Episode 48/50 complete. Score: -0.426. Decisions: 0\n",
      "  Episode 49/50 complete. Score: 0.096. Decisions: 0\n",
      "  Episode 50/50 complete. Score: 0.045. Decisions: 0\n",
      "Average episode score: 0.3037\n",
      "Total decisions added to buffer: 0\n",
      "Average training loss: 1.0432\n",
      "Average episode size: 69.6326\n",
      "\n",
      "--- TRAINING LOOP 10/100 ---\n",
      "  Episode 1/50 complete. Score: 0.626. Decisions: 0\n",
      "  Episode 2/50 complete. Score: 0.642. Decisions: 0\n",
      "  Episode 3/50 complete. Score: 0.666. Decisions: 0\n",
      "  Episode 4/50 complete. Score: 0.469. Decisions: 0\n",
      "  Episode 5/50 complete. Score: 0.184. Decisions: 0\n",
      "  Episode 6/50 complete. Score: 0.528. Decisions: 0\n",
      "  Episode 7/50 complete. Score: 0.444. Decisions: 0\n",
      "  Episode 8/50 complete. Score: 0.107. Decisions: 0\n",
      "  Episode 9/50 complete. Score: 0.080. Decisions: 0\n",
      "  Episode 10/50 complete. Score: 0.193. Decisions: 0\n",
      "  Episode 11/50 complete. Score: 0.478. Decisions: 0\n",
      "  Episode 12/50 complete. Score: -0.113. Decisions: 0\n",
      "  Episode 13/50 complete. Score: 0.523. Decisions: 0\n",
      "  Episode 14/50 complete. Score: 0.314. Decisions: 0\n",
      "  Episode 15/50 complete. Score: 0.595. Decisions: 0\n",
      "  Episode 16/50 complete. Score: -0.161. Decisions: 0\n",
      "  Episode 17/50 complete. Score: 0.468. Decisions: 0\n",
      "  Episode 18/50 complete. Score: 0.625. Decisions: 0\n",
      "  Episode 19/50 complete. Score: 0.405. Decisions: 0\n",
      "  Episode 20/50 complete. Score: 0.528. Decisions: 0\n",
      "  Episode 21/50 complete. Score: 0.485. Decisions: 0\n",
      "  Episode 22/50 complete. Score: 0.627. Decisions: 0\n",
      "  Episode 23/50 complete. Score: 0.536. Decisions: 0\n",
      "  Episode 24/50 complete. Score: -0.082. Decisions: 0\n",
      "  Episode 25/50 complete. Score: -0.071. Decisions: 0\n",
      "  Episode 26/50 complete. Score: 0.251. Decisions: 0\n",
      "  Episode 27/50 complete. Score: -0.110. Decisions: 0\n",
      "  Episode 28/50 complete. Score: -0.033. Decisions: 0\n",
      "  Episode 29/50 complete. Score: 0.151. Decisions: 0\n",
      "  Episode 30/50 complete. Score: 0.551. Decisions: 0\n",
      "  Episode 31/50 complete. Score: -0.255. Decisions: 0\n",
      "  Episode 32/50 complete. Score: 0.463. Decisions: 0\n",
      "  Episode 33/50 complete. Score: 0.524. Decisions: 0\n",
      "  Episode 34/50 complete. Score: 0.527. Decisions: 0\n",
      "  Episode 35/50 complete. Score: 0.354. Decisions: 0\n",
      "  Episode 36/50 complete. Score: 0.575. Decisions: 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(CONFIG)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Starting Training Loop ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_training_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Training Complete ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[18], line 652\u001b[0m, in \u001b[0;36mTrainer.run_training_loop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    650\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepisodes_per_loop\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n\u001b[1;32m    651\u001b[0m     buffer_before \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplay_buffer)\n\u001b[0;32m--> 652\u001b[0m     score, raw \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_simulation_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    653\u001b[0m     buffer_after \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplay_buffer)\n\u001b[1;32m    655\u001b[0m     decisions \u001b[38;5;241m=\u001b[39m buffer_after \u001b[38;5;241m-\u001b[39m buffer_before\n",
      "Cell \u001b[0;32mIn[18], line 547\u001b[0m, in \u001b[0;36mTrainer.run_simulation_episode\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    543\u001b[0m free_servers \u001b[38;5;241m=\u001b[39m net\u001b[38;5;241m.\u001b[39m_get_free_servers()\n\u001b[1;32m    545\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m free_servers:\n\u001b[1;32m    546\u001b[0m     \u001b[38;5;66;03m# 1. A decision is needed. Call MCTS.\u001b[39;00m\n\u001b[0;32m--> 547\u001b[0m     assignments, root, state_vec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmcts_policy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecide\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    548\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfree_servers\u001b[49m\n\u001b[1;32m    549\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;66;03m# 2. Get the training data\u001b[39;00m\n\u001b[1;32m    552\u001b[0m     policy_target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmcts_policy\u001b[38;5;241m.\u001b[39mget_policy_target(\n\u001b[1;32m    553\u001b[0m         root, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    554\u001b[0m     )\n",
      "Cell \u001b[0;32mIn[18], line 337\u001b[0m, in \u001b[0;36mMCTS_Policy.decide\u001b[0;34m(self, net, t, free_servers)\u001b[0m\n\u001b[1;32m    335\u001b[0m                     policy_probs[i] \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m eps) \u001b[38;5;241m*\u001b[39m policy_probs[i] \u001b[38;5;241m+\u001b[39m eps \u001b[38;5;241m*\u001b[39m noise[idx]\n\u001b[1;32m    336\u001b[0m                     idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 337\u001b[0m     \u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpolicy_probs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;66;03m# --- BACKPROPAGATION ---\u001b[39;00m\n\u001b[1;32m    340\u001b[0m node\u001b[38;5;241m.\u001b[39mbackpropagate(value)\n",
      "Cell \u001b[0;32mIn[18], line 148\u001b[0m, in \u001b[0;36mMCTSNode.expand\u001b[0;34m(self, policy_probs)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Expand this node using NN policy priors.\"\"\"\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m action_idx, prob \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(policy_probs):\n\u001b[0;32m--> 148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m prob \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m: \u001b[38;5;66;03m# Only create nodes for legal actions\u001b[39;00m\n\u001b[1;32m    149\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m action_idx \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren:\n\u001b[1;32m    150\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren[action_idx] \u001b[38;5;241m=\u001b[39m MCTSNode(parent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, prior_p\u001b[38;5;241m=\u001b[39mprob)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    print(\"Initializing Trainer...\")\n",
    "    trainer = Trainer(CONFIG)\n",
    "    \n",
    "    print(\"\\n--- Starting Training Loop ---\")\n",
    "    trainer.run_training_loop()\n",
    "    \n",
    "    print(\"\\n--- Training Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b296ce90-3bb7-4f44-a858-b364bc9deb40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
